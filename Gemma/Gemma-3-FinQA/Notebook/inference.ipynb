{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f042ad7-f1af-4a61-9977-df64046f5db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unsloth in /usr/local/lib/python3.10/dist-packages (2025.4.7)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
      "Requirement already satisfied: xformers in /usr/local/lib/python3.10/dist-packages (0.0.30)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.30.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.7.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.7.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.5)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
      "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.19.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: unsloth_zoo>=2025.4.4 in /usr/local/lib/python3.10/dist-packages (from unsloth) (2025.4.4)\n",
      "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth) (23.2)\n",
      "Requirement already satisfied: tyro in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.9.19)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth) (5.9.6)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.45.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.24.1)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.6.0)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.20.3)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.1.9)\n",
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.33.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.10/dist-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.10/dist-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.10/dist-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton>=3.0.0->unsloth) (68.2.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (14.0.0)\n",
      "Collecting click!=8.0.0,>=7.1 (from wandb)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (3.11.0)\n",
      "Collecting pydantic<3 (from wandb)\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.27.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.20.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3->wandb)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3->wandb)\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3->wandb)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2025.4.4->unsloth) (25.1.1)\n",
      "Requirement already satisfied: msgspec in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2025.4.4->unsloth) (0.19.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from diffusers->unsloth) (4.6.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.16.1)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (1.7.2)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (4.4.2)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "\u001b[33mWARNING: huggingface-hub 0.30.2 does not provide the extra 'hf-xet'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Downloading wandb-0.19.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m213.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m443.9/443.9 kB\u001b[0m \u001b[31m160.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m238.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.27.0-py2.py3-none-any.whl (340 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m340.8/340.8 kB\u001b[0m \u001b[31m147.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: typing-inspection, smmap, setproctitle, sentry-sdk, pydantic-core, docker-pycreds, click, annotated-types, pydantic, gitdb, gitpython, wandb\n",
      "Successfully installed annotated-types-0.7.0 click-8.1.8 docker-pycreds-0.4.0 gitdb-4.0.12 gitpython-3.1.44 pydantic-2.11.4 pydantic-core-2.33.2 sentry-sdk-2.27.0 setproctitle-1.3.6 smmap-5.0.2 typing-inspection-0.4.0 wandb-0.19.10\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install unsloth transformers datasets xformers huggingface_hub torch torchvision torchaudio bitsandbytes peft trl tiktoken wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bd99b7-65fe-4717-a6a6-1d32572dc89c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17b7dc31-b669-463f-add4-02cb0cf68de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f768b089d5e646a184653ac55ad50f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8ccd92-72bd-4745-a84a-75bcad235ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81bbadeb-ba72-491a-92dc-fd178906f4df",
   "metadata": {},
   "source": [
    "# WandB init()\n",
    "init(): https://docs.wandb.ai/ref/python/init/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08dbf13a-8403-4ff0-a26f-047a18d85ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlaravelshubham\u001b[0m (\u001b[33mlaravelshubham-student\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e5854d0-b741-41b6-b201-2e96a35cd96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/Ft_Gemma/wandb/run-20250504_175237-xi4dson4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/laravelshubham-student/Fine-tuning%20Gemma-3-4B%20on%20FinQA%20Reasoning%20Dataset/runs/xi4dson4?apiKey=9cec55360d515cebf3a98e5f8de569b5c44f6265' target=\"_blank\">star-bantha-1</a></strong> to <a href='https://wandb.ai/laravelshubham-student/Fine-tuning%20Gemma-3-4B%20on%20FinQA%20Reasoning%20Dataset?apiKey=9cec55360d515cebf3a98e5f8de569b5c44f6265' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/laravelshubham-student/Fine-tuning%20Gemma-3-4B%20on%20FinQA%20Reasoning%20Dataset?apiKey=9cec55360d515cebf3a98e5f8de569b5c44f6265' target=\"_blank\">https://wandb.ai/laravelshubham-student/Fine-tuning%20Gemma-3-4B%20on%20FinQA%20Reasoning%20Dataset?apiKey=9cec55360d515cebf3a98e5f8de569b5c44f6265</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/laravelshubham-student/Fine-tuning%20Gemma-3-4B%20on%20FinQA%20Reasoning%20Dataset/runs/xi4dson4?apiKey=9cec55360d515cebf3a98e5f8de569b5c44f6265' target=\"_blank\">https://wandb.ai/laravelshubham-student/Fine-tuning%20Gemma-3-4B%20on%20FinQA%20Reasoning%20Dataset/runs/xi4dson4?apiKey=9cec55360d515cebf3a98e5f8de569b5c44f6265</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Do NOT share these links with anyone. They can be used to claim your runs."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project='Fine-tuning Gemma-3-4B on FinQA Reasoning Dataset', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700bce0e-447d-4fcf-9230-acdcb0dc651a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31d1cfb2-ffa1-4622-afb0-a433eec14da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: We'll be using `/tmp/unsloth_compiled_cache` for temporary Unsloth patches.\n",
      "Standard import failed for UnslothCPOTrainer: No module named 'UnslothCPOTrainer'. Using tempfile instead!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2439cbf-c52b-4aff-90d5-9e0f7c63b368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.7: Fast Gemma3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA RTX 6000 Ada Generation. Num GPUs = 1. Max memory: 47.5 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c3479c9bc540dea8928794b4947a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d8a2c77d014cdda94494367d37fced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41afe9a073bb445bb8aa028568650597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625a72d101004959bb26768a7bc8437e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b20223e7ed84c52a2ca0a3cb92e6320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc11661341640a79200663b688979f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5c7832a0344ba0b577fe3eae7e1c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f2e90f710742fb85e7f2139ecc8aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e104ae0a3742c0bee218b920e332e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8460006a4be4488ba906a9f26483790b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb79dc107c534af1b28e207c948aaa66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it\",\n",
    "    max_seq_length = 2048, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6bbf2e-0f62-4318-ad59-ae25942f7b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bfe70d1-e57f-4b67-8c49-9ce4407038c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForConditionalGeneration(\n",
       "  (vision_tower): SiglipVisionModel(\n",
       "    (vision_model): SiglipVisionTransformer(\n",
       "      (embeddings): SiglipVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "        (position_embedding): Embedding(4096, 1152)\n",
       "      )\n",
       "      (encoder): SiglipEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-26): 27 x SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): PytorchGELUTanh()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multi_modal_projector): Gemma3MultiModalProjector(\n",
       "    (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "    (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "  )\n",
       "  (language_model): Gemma3ForCausalLM(\n",
       "    (model): Gemma3TextModel(\n",
       "      (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Gemma3DecoderLayer(\n",
       "          (self_attn): Gemma3Attention(\n",
       "            (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Gemma3MLP(\n",
       "            (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "        )\n",
       "        (2): Gemma3DecoderLayer(\n",
       "          (self_attn): Gemma3Attention(\n",
       "            (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
       "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Gemma3MLP(\n",
       "            (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "        )\n",
       "        (3-6): 4 x Gemma3DecoderLayer(\n",
       "          (self_attn): Gemma3Attention(\n",
       "            (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Gemma3MLP(\n",
       "            (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "        )\n",
       "        (7-33): 27 x Gemma3DecoderLayer(\n",
       "          (self_attn): Gemma3Attention(\n",
       "            (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Gemma3MLP(\n",
       "            (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "      (rotary_emb): Gemma3RotaryEmbedding()\n",
       "      (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e5533f-4d10-40eb-8fa9-b5f55ca2e12b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de066b6c-15c6-402b-a2f7-3008cbe2c6ce",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84fac3e6-e8ac-4b41-b13e-38e2db1471dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "There is a Question provided below. It has an instruction that describes a task, paired with the input that provides further context to the task.\n",
    "Write a response that appropriately completes the request.\n",
    "\n",
    "Before providing the response, think about the question carefully and then create a step-by-step chain of thoughts to ensure that the response is logical and accurate.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "{}\n",
    "\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b47584b-05a5-4373-909e-6a97b72820e4",
   "metadata": {},
   "source": [
    "Dataset: https://huggingface.co/datasets/TheFinAI/Fino1_Reasoning_Path_FinQA/viewer?views%5B%5D=train&sql=--+The+SQL+console+is+powered+by+DuckDB+WASM+and+runs+entirely+in+the+browser.%0A--+Get+started+by+typing+a+query+or+selecting+a+view+from+the+options+below.%0ASELECT+*+FROM+train+LIMIT+1%3B&sql_row=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44e19ea1-8839-4b75-8c55-a145a170860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Please answer the given financial question based on the context.\n",
    "Context: amortization expense , which is included in selling , general and administrative expenses , was $ 13.0 million , $ 13.9 million and $ 8.5 million for the years ended december 31 , 2016 , 2015 and 2014 , respectively . the following is the estimated amortization expense for the company 2019s intangible assets as of december 31 , 2016 : ( in thousands ) .\n",
    "|2017|$ 10509|\n",
    "|2018|9346|\n",
    "|2019|9240|\n",
    "|2020|7201|\n",
    "|2021|5318|\n",
    "|2022 and thereafter|16756|\n",
    "|amortization expense of intangible assets|$ 58370|\n",
    "at december 31 , 2016 , 2015 and 2014 , the company determined that its goodwill and indefinite- lived intangible assets were not impaired . 5 . credit facility and other long term debt credit facility the company is party to a credit agreement that provides revolving commitments for up to $ 1.25 billion of borrowings , as well as term loan commitments , in each case maturing in january 2021 . as of december 31 , 2016 there was no outstanding balance under the revolving credit facility and $ 186.3 million of term loan borrowings remained outstanding . at the company 2019s request and the lender 2019s consent , revolving and or term loan borrowings may be increased by up to $ 300.0 million in aggregate , subject to certain conditions as set forth in the credit agreement , as amended . incremental borrowings are uncommitted and the availability thereof , will depend on market conditions at the time the company seeks to incur such borrowings . the borrowings under the revolving credit facility have maturities of less than one year . up to $ 50.0 million of the facility may be used for the issuance of letters of credit . there were $ 2.6 million of letters of credit outstanding as of december 31 , 2016 . the credit agreement contains negative covenants that , subject to significant exceptions , limit the ability of the company and its subsidiaries to , among other things , incur additional indebtedness , make restricted payments , pledge their assets as security , make investments , loans , advances , guarantees and acquisitions , undergo fundamental changes and enter into transactions with affiliates . the company is also required to maintain a ratio of consolidated ebitda , as defined in the credit agreement , to consolidated interest expense of not less than 3.50 to 1.00 and is not permitted to allow the ratio of consolidated total indebtedness to consolidated ebitda to be greater than 3.25 to 1.00 ( 201cconsolidated leverage ratio 201d ) . as of december 31 , 2016 , the company was in compliance with these ratios . in addition , the credit agreement contains events of default that are customary for a facility of this nature , and includes a cross default provision whereby an event of default under other material indebtedness , as defined in the credit agreement , will be considered an event of default under the credit agreement . borrowings under the credit agreement bear interest at a rate per annum equal to , at the company 2019s option , either ( a ) an alternate base rate , or ( b ) a rate based on the rates applicable for deposits in the interbank market for u.s . dollars or the applicable currency in which the loans are made ( 201cadjusted libor 201d ) , plus in each case an applicable margin . the applicable margin for loans will .\n",
    "Question: what portion of the estimated amortization expense will be recognized in 2017?\n",
    "Answer:\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9de447-8b04-44c9-83c7-29f1f610a102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "087bf0cb-0060-4fdd-9669-bd1df65912bf",
   "metadata": {},
   "source": [
    "## Enable Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93fbd1e8-e6d8-4734-866c-ad78efdd7a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForConditionalGeneration(\n",
       "  (vision_tower): SiglipVisionModel(\n",
       "    (vision_model): SiglipVisionTransformer(\n",
       "      (embeddings): SiglipVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "        (position_embedding): Embedding(4096, 1152)\n",
       "      )\n",
       "      (encoder): SiglipEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-26): 27 x SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): PytorchGELUTanh()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multi_modal_projector): Gemma3MultiModalProjector(\n",
       "    (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "    (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "  )\n",
       "  (language_model): Gemma3ForCausalLM(\n",
       "    (model): Gemma3TextModel(\n",
       "      (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Gemma3DecoderLayer(\n",
       "          (self_attn): Gemma3Attention(\n",
       "            (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Gemma3MLP(\n",
       "            (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "        )\n",
       "        (2): Gemma3DecoderLayer(\n",
       "          (self_attn): Gemma3Attention(\n",
       "            (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
       "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Gemma3MLP(\n",
       "            (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "        )\n",
       "        (3-6): 4 x Gemma3DecoderLayer(\n",
       "          (self_attn): Gemma3Attention(\n",
       "            (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Gemma3MLP(\n",
       "            (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "        )\n",
       "        (7-33): 27 x Gemma3DecoderLayer(\n",
       "          (self_attn): Gemma3Attention(\n",
       "            (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Gemma3MLP(\n",
       "            (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "      (rotary_emb): Gemma3RotaryEmbedding()\n",
       "      (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastModel.for_inference(model) # Enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22616c4a-43e9-4c25-a547-1193a76d4528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e55f8adf-be20-44ad-bd00-05a2339e683e",
   "metadata": {},
   "source": [
    "### Format the prompt\n",
    "\n",
    "This is a call to Pythonâ€™s .format() string method, used to substitute values into a predefined template string.\n",
    "\n",
    "In this case, you have a prompt template (likely a string) with placeholders â€” like:\n",
    "```\n",
    "prompt_style = \"Q: {}\\nA: {}\"\n",
    "```\n",
    "Then you call:\n",
    "```\n",
    "prompt_style.format(question, \"\")\n",
    "```\n",
    "This will substitute:\n",
    "```\n",
    "The first {} with question\n",
    "\n",
    "The second {} with \"\" (an empty string â€” maybe for the answer)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "043ff71e-cd31-472b-9aa4-0b3bb0c7cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_format = prompt.format(question, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1a0c124-57f5-4e9c-aac8-605cc77ea01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is a Question provided below. It has an instruction that describes a task, paired with the input that provides further context to the task.\\nWrite a response that appropriately completes the request.\\n\\nBefore providing the response, think about the question carefully and then create a step-by-step chain of thoughts to ensure that the response is logical and accurate.\\n\\n### Question:\\nPlease answer the given financial question based on the context.\\nContext: amortization expense , which is included in selling , general and administrative expenses , was $ 13.0 million , $ 13.9 million and $ 8.5 million for the years ended december 31 , 2016 , 2015 and 2014 , respectively . the following is the estimated amortization expense for the company 2019s intangible assets as of december 31 , 2016 : ( in thousands ) .\\n|2017|$ 10509|\\n|2018|9346|\\n|2019|9240|\\n|2020|7201|\\n|2021|5318|\\n|2022 and thereafter|16756|\\n|amortization expense of intangible assets|$ 58370|\\nat december 31 , 2016 , 2015 and 2014 , the company determined that its goodwill and indefinite- lived intangible assets were not impaired . 5 . credit facility and other long term debt credit facility the company is party to a credit agreement that provides revolving commitments for up to $ 1.25 billion of borrowings , as well as term loan commitments , in each case maturing in january 2021 . as of december 31 , 2016 there was no outstanding balance under the revolving credit facility and $ 186.3 million of term loan borrowings remained outstanding . at the company 2019s request and the lender 2019s consent , revolving and or term loan borrowings may be increased by up to $ 300.0 million in aggregate , subject to certain conditions as set forth in the credit agreement , as amended . incremental borrowings are uncommitted and the availability thereof , will depend on market conditions at the time the company seeks to incur such borrowings . the borrowings under the revolving credit facility have maturities of less than one year . up to $ 50.0 million of the facility may be used for the issuance of letters of credit . there were $ 2.6 million of letters of credit outstanding as of december 31 , 2016 . the credit agreement contains negative covenants that , subject to significant exceptions , limit the ability of the company and its subsidiaries to , among other things , incur additional indebtedness , make restricted payments , pledge their assets as security , make investments , loans , advances , guarantees and acquisitions , undergo fundamental changes and enter into transactions with affiliates . the company is also required to maintain a ratio of consolidated ebitda , as defined in the credit agreement , to consolidated interest expense of not less than 3.50 to 1.00 and is not permitted to allow the ratio of consolidated total indebtedness to consolidated ebitda to be greater than 3.25 to 1.00 ( 201cconsolidated leverage ratio 201d ) . as of december 31 , 2016 , the company was in compliance with these ratios . in addition , the credit agreement contains events of default that are customary for a facility of this nature , and includes a cross default provision whereby an event of default under other material indebtedness , as defined in the credit agreement , will be considered an event of default under the credit agreement . borrowings under the credit agreement bear interest at a rate per annum equal to , at the company 2019s option , either ( a ) an alternate base rate , or ( b ) a rate based on the rates applicable for deposits in the interbank market for u.s . dollars or the applicable currency in which the loans are made ( 201cadjusted libor 201d ) , plus in each case an applicable margin . the applicable margin for loans will .\\nQuestion: what portion of the estimated amortization expense will be recognized in 2017?\\nAnswer:\\n\\n### Response:\\n<think>\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c20787-f7dc-460b-8e9a-8e4ca78a9d87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a740012-36c7-4cb0-ac23-2241c61557bc",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "https://huggingface.co/docs/tokenizers/training_from_memory\n",
    "\n",
    "ðŸ§  Conceptual Overview: What is the Unsloth Tokenizer?\n",
    "\n",
    "The tokenizer in Unsloth is typically:\n",
    "\n",
    "1. Derived from Hugging Faceâ€™s tokenizer (like LLaMATokenizer, AutoTokenizer)\n",
    "\n",
    "2. Wrapped with extra utilities to support:\n",
    "\n",
    "    * Instruction tuning (chat format)\n",
    "\n",
    "    * Vision + Text tokenization\n",
    "\n",
    "    * Token-efficient formatting\n",
    "\n",
    "    * Faster .to() support with .cuda() or .to(device)\n",
    "\n",
    "ðŸ› ï¸ How Unsloth Tokenizer Works Under the Hood\n",
    "\n",
    "When you call:\n",
    "```\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "```\n",
    "This typically:\n",
    "1. Tokenizes your text prompt\n",
    "2. Pads/truncates according to max_length\n",
    "3. Converts to PyTorch tensors\n",
    "4. Moves them to GPU (.to(\"cuda\"))\n",
    "\n",
    "âœ¨ Example Output\n",
    "```\n",
    "{\n",
    "  'input_ids': tensor([[1, 29871, 318, 1234, 456, 2]]),\n",
    "  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "  'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])\n",
    "}\n",
    "```\n",
    "For vision inputs:\n",
    "```\n",
    "inputs = tokenizer(image, text, return_tensors=\"pt\")\n",
    "```\n",
    "It will return:\n",
    "```\n",
    "{\n",
    "  'pixel_values': tensor(...),          # image embedding\n",
    "  'input_ids': tensor(...),            # tokenized prompt\n",
    "  'attention_mask': tensor(...)\n",
    "}\n",
    "```\n",
    "ðŸ“¦ Special Features in Unslothâ€™s Tokenizer\n",
    "Feature: Description\n",
    "\n",
    "1. apply_chat_template(): Converts instruction + response into formatted prompt\n",
    "2. to(device): Moves all tensors to a device in one line\n",
    "3. Multimodal support: Accepts image + text (if Vision model is used)\n",
    "4. Fast tokenization: Tokenizers may be patched for performance (faster than Hugging Face baseline)\n",
    "5. Instruction formatting: Integrates system, user, assistant roles for chat training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4638210f-f3d3-43fa-8978-1d209fd74a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([prompt_format], return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f04df38b-8b10-4c5f-a78f-51785c8c0a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     2,   3810,    563,    496,  19566,   3847,   3426, 236761,   1030,\n",
       "            815,    614,  14787,    600,  15517,    496,   4209, 236764,  33481,\n",
       "            607,    506,   2744,    600,   4728,   3342,   4403,    531,    506,\n",
       "           4209, 236761,    107,   6974,    496,   3072,    600,  37404,  42342,\n",
       "            506,   2864, 236761,    108,  13286,   6655,    506,   3072, 236764,\n",
       "           1751,   1003,    506,   2934,  13058,    532,   1299,   2619,    496,\n",
       "           2918, 236772,   2003, 236772,   9340,   7797,    529,  12018,    531,\n",
       "           5330,    600,    506,   3072,    563,  23420,    532,  11459, 236761,\n",
       "            108,  10354,  19566, 236787,    107,   9366,   3890,    506,   2238,\n",
       "           4867,   2934,   2721,    580,    506,   4403, 236761,    107,   3637,\n",
       "         236787, 190375,  18766,   1031,    837,    563,   5012,    528,  11010,\n",
       "           1031,   2870,    532,  15006,  14348,   1031,    691,    609, 236743,\n",
       "         236770, 236800, 236761, 236771,   3625,   1031,    609, 236743, 236770,\n",
       "         236800, 236761, 236819,   3625,    532,    609, 236743, 236828, 236761,\n",
       "         236810,   3625,    573,    506,   1518,  10714,  45300, 236743, 236800,\n",
       "         236770,   1031, 236743, 236778, 236771, 236770, 236825,   1031, 236743,\n",
       "         236778, 236771, 236770, 236810,    532, 236743, 236778, 236771, 236770,\n",
       "         236812,   1031,   6619,    783,    506,   2269,    563,    506,   9460,\n",
       "         190375,  18766,    573,    506,   2544, 236743, 236778, 236771, 236770,\n",
       "         236819, 236751, 106288,  10091,    618,    529,  45300, 236743, 236800,\n",
       "         236770,   1031, 236743, 236778, 236771, 236770, 236825,   1017,    568,\n",
       "            528,  11252,   1732,    783,    107, 236909, 236778, 236771, 236770,\n",
       "         236832,  43043, 236743, 236770, 236771, 236810, 236771, 236819, 236909,\n",
       "            107, 236909, 236778, 236771, 236770, 236828, 236909, 236819, 236800,\n",
       "         236812, 236825, 236909,    107, 236909, 236778, 236771, 236770, 236819,\n",
       "         236909, 236819, 236778, 236812, 236771, 236909,    107, 236909, 236778,\n",
       "         236771, 236778, 236771, 236909, 236832, 236778, 236771, 236770, 236909,\n",
       "            107, 236909, 236778, 236771, 236778, 236770, 236909, 236810, 236800,\n",
       "         236770, 236828, 236909,    107, 236909, 236778, 236771, 236778, 236778,\n",
       "            532,  34324, 236909, 236770, 236825, 236832, 236810, 236825, 236909,\n",
       "            107, 236909, 213235,   1854,  18766,    529, 106288,  10091,  43043,\n",
       "         236743, 236810, 236828, 236800, 236832, 236771, 236909,    107,    502,\n",
       "          45300, 236743, 236800, 236770,   1031, 236743, 236778, 236771, 236770,\n",
       "         236825,   1031, 236743, 236778, 236771, 236770, 236810,    532, 236743,\n",
       "         236778, 236771, 236770, 236812,   1031,    506,   2544,   6185,    600,\n",
       "           1061,  91205,    532,  78986, 236772,  11742, 106288,  10091,    964,\n",
       "            711,  42236,    783, 236743, 236810,    783,   6327,  10056,    532,\n",
       "           1032,   1440,   1941,   8479,   6327,  10056,    506,   2544,    563,\n",
       "           4598,    531,    496,   6327,   7686,    600,   4728,  80763,  41716,\n",
       "            573,    872,    531,    609, 236743, 236770, 236761, 236778, 236810,\n",
       "           7889,    529, 221175,   1031,    618,   1388,    618,   1941,  10670,\n",
       "          41716,   1031,    528,   1546,   1624, 116167,    528,  68074, 236743,\n",
       "         236778, 236771, 236778, 236770,    783,    618,    529,  45300, 236743,\n",
       "         236800, 236770,   1031, 236743, 236778, 236771, 236770, 236825,    993,\n",
       "            691,    951,  15647,   7002,   1208,    506,  80763,   6327,  10056,\n",
       "            532,    609, 236743, 236770, 236828, 236825, 236761, 236800,   3625,\n",
       "            529,   1941,  10670, 221175,  12800,  15647,    783,    657,    506,\n",
       "           2544, 236743, 236778, 236771, 236770, 236819, 236751,   2864,    532,\n",
       "            506,  40739, 236743, 236778, 236771, 236770, 236819, 236751,  13782,\n",
       "           1031,  80763,    532,    653,   1941,  10670, 221175,   1149,    577,\n",
       "           4869,    684,    872,    531,    609, 236743, 236800, 236771, 236771,\n",
       "         236761, 236771,   3625,    528,  27834,   1031,   3660,    531,   2953,\n",
       "           3439,    618,   1076,  12034,    528,    506,   6327,   7686,   1031,\n",
       "            618,  22474,    783,  56296, 221175,    659,    723, 157407,    532,\n",
       "            506,  15830,  17344,   1031,    795,   4911,    580,   2436,   3439,\n",
       "            657,    506,    990,    506,   2544,  26469,    531,  24397,   1288,\n",
       "         221175,    783,    506, 221175,   1208,    506,  80763,   6327,  10056,\n",
       "            735, 225877,    529,   2344,   1082,    886,   1051,    783,    872,\n",
       "            531,    609, 236743, 236810, 236771, 236761, 236771,   3625,    529,\n",
       "            506,  10056,   1149,    577,   1456,    573,    506,  50658,    529,\n",
       "          11739,    529,   6327,    783,    993,    964,    609, 236743, 236778,\n",
       "         236761, 236825,   3625,    529,  11739,    529,   6327,  15647,    618,\n",
       "            529,  45300, 236743, 236800, 236770,   1031, 236743, 236778, 236771,\n",
       "         236770, 236825,    783,    506,   6327,   7686,   6097,   5676, 147343,\n",
       "            600,   1031,   3660,    531,   3629,  25920,   1031,   4576,    506,\n",
       "           5596,    529,    506,   2544,    532,   1061,  84318,    531,   1031,\n",
       "           3571,   1032,   2432,   1031,  24397,   4767,  79783,   1031,   1386,\n",
       "          19188,  13192,   1031,  53280,    910,  10091,    618,   5052,   1031,\n",
       "           1386,  17461,   1031,  15682,   1031,  28051,   1031,  36369,    532,\n",
       "          55171,   1031,  26833,  11001,   3731,    532,   5273,   1131,  16788,\n",
       "            607,  71978,    783,    506,   2544,    563,    992,   3149,    531,\n",
       "           4883,    496,   7232,    529,  53880,    545,   3896,   1926,   1031,\n",
       "            618,   5221,    528,    506,   6327,   7686,   1031,    531,  53880,\n",
       "           2142,  18766,    529,    711,   2344,   1082, 236743, 236800, 236761,\n",
       "         236810, 236771,    531, 236743, 236770, 236761, 236771, 236771,    532,\n",
       "            563,    711,  18345,    531,   2174,    506,   7232,    529,  53880,\n",
       "           2558,  79783,    531,  53880,    545,   3896,   1926,    531,    577,\n",
       "           5314,   1082, 236743, 236800, 236761, 236778, 236810,    531, 236743,\n",
       "         236770, 236761, 236771, 236771,    568, 236743, 236778, 236771, 236770,\n",
       "         236755, 229553,    774,  30990,   7232, 236743, 236778, 236771, 236770,\n",
       "         236753,   1732,    783,    618,    529,  45300, 236743, 236800, 236770,\n",
       "           1031, 236743, 236778, 236771, 236770, 236825,   1031,    506,   2544,\n",
       "            691,    528,  14143,    607,   1239,  24781,    783,    528,   2517,\n",
       "           1031,    506,   6327,   7686,   6097,   4749,    529,   3678,    600,\n",
       "            659,  57487,    573,    496,  10056,    529,    672,   4135,   1031,\n",
       "            532,   5132,    496,   4071,   3678,  13313,  36849,    614,   2460,\n",
       "            529,   3678,   1208,   1032,   2561,  79783,   1031,    618,   5221,\n",
       "            528,    506,   6327,   7686,   1031,    795,    577,   4542,    614,\n",
       "           2460,    529,   3678,   1208,    506,   6327,   7686,    783, 221175,\n",
       "           1208,    506,   6327,   7686,  10591,   2142,    657,    496,   3136,\n",
       "            810,  54224,   4745,    531,   1031,    657,    506,   2544, 236743,\n",
       "         236778, 236771, 236770, 236819, 236751,   5086,   1031,   3477,    568,\n",
       "            496,   1732,    614,  28540,   3225,   3136,   1031,    653,    568,\n",
       "            518,   1732,    496,   3136,   2721,    580,    506,   6224,  11955,\n",
       "            573,  22050,    528,    506,    939,  15433,   2436,    573,    559,\n",
       "         236761, 236751,    783,  11092,    653,    506,  11955,  15130,    528,\n",
       "            837,    506,  15682,    659,   1603,    568, 236743, 236778, 236771,\n",
       "         236770,  27174,   4017,    524,   4758,    504, 236743, 236778, 236771,\n",
       "         236770, 236753,   1732,   1031,   2915,    528,   1546,   1624,    614,\n",
       "          11955,   5416,    783,    506,  11955,   5416,    573,  15682,    795,\n",
       "            783,    107,  14977, 236787,   1144,   9381,    529,    506,   9460,\n",
       "         190375,  18766,    795,    577,  12659,    528, 236743, 236778, 236771,\n",
       "         236770, 236832, 236881,    107,   7925, 236787,    108,  10354,  14503,\n",
       "         236787,    107, 236820,  36345, 236813,    107]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef68ba9-57ef-4190-affd-5969e5164166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8914a80b-cd8e-447f-a98f-ea681423df7c",
   "metadata": {},
   "source": [
    "### Generate\n",
    "\n",
    "For decoder-only models inputs should be in the format of input_ids. For encoder-decoder models inputs can represent any of input_ids, input_values, input_features, or pixel_values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ee7dcad-d392-47ad-9da7-74861cc003a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1024,\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba8644e5-281c-4b06-8317-47a8d2350505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2,   3810,    563,  ..., 236819,    107,    106]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed1967c4-259f-4917-9e6b-b1eff72f0a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0698600-a72b-4a8b-8ec3-8eda1ea83d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos>There is a Question provided below. It has an instruction that describes a task, paired with the input that provides further context to the task.\\nWrite a response that appropriately completes the request.\\n\\nBefore providing the response, think about the question carefully and then create a step-by-step chain of thoughts to ensure that the response is logical and accurate.\\n\\n### Question:\\nPlease answer the given financial question based on the context.\\nContext: amortization expense , which is included in selling , general and administrative expenses , was $ 13.0 million , $ 13.9 million and $ 8.5 million for the years ended december 31 , 2016 , 2015 and 2014 , respectively . the following is the estimated amortization expense for the company 2019s intangible assets as of december 31 , 2016 : ( in thousands ) .\\n|2017|$ 10509|\\n|2018|9346|\\n|2019|9240|\\n|2020|7201|\\n|2021|5318|\\n|2022 and thereafter|16756|\\n|amortization expense of intangible assets|$ 58370|\\nat december 31 , 2016 , 2015 and 2014 , the company determined that its goodwill and indefinite- lived intangible assets were not impaired . 5 . credit facility and other long term debt credit facility the company is party to a credit agreement that provides revolving commitments for up to $ 1.25 billion of borrowings , as well as term loan commitments , in each case maturing in january 2021 . as of december 31 , 2016 there was no outstanding balance under the revolving credit facility and $ 186.3 million of term loan borrowings remained outstanding . at the company 2019s request and the lender 2019s consent , revolving and or term loan borrowings may be increased by up to $ 300.0 million in aggregate , subject to certain conditions as set forth in the credit agreement , as amended . incremental borrowings are uncommitted and the availability thereof , will depend on market conditions at the time the company seeks to incur such borrowings . the borrowings under the revolving credit facility have maturities of less than one year . up to $ 50.0 million of the facility may be used for the issuance of letters of credit . there were $ 2.6 million of letters of credit outstanding as of december 31 , 2016 . the credit agreement contains negative covenants that , subject to significant exceptions , limit the ability of the company and its subsidiaries to , among other things , incur additional indebtedness , make restricted payments , pledge their assets as security , make investments , loans , advances , guarantees and acquisitions , undergo fundamental changes and enter into transactions with affiliates . the company is also required to maintain a ratio of consolidated ebitda , as defined in the credit agreement , to consolidated interest expense of not less than 3.50 to 1.00 and is not permitted to allow the ratio of consolidated total indebtedness to consolidated ebitda to be greater than 3.25 to 1.00 ( 201cconsolidated leverage ratio 201d ) . as of december 31 , 2016 , the company was in compliance with these ratios . in addition , the credit agreement contains events of default that are customary for a facility of this nature , and includes a cross default provision whereby an event of default under other material indebtedness , as defined in the credit agreement , will be considered an event of default under the credit agreement . borrowings under the credit agreement bear interest at a rate per annum equal to , at the company 2019s option , either ( a ) an alternate base rate , or ( b ) a rate based on the rates applicable for deposits in the interbank market for u.s . dollars or the applicable currency in which the loans are made ( 201cadjusted libor 201d ) , plus in each case an applicable margin . the applicable margin for loans will .\\nQuestion: what portion of the estimated amortization expense will be recognized in 2017?\\nAnswer:\\n\\n### Response:\\n<think>\\nThe question asks about the portion of the estimated amortization expense that will be recognized in 2017.\\nThe context provides the estimated amortization expense for intangible assets for the years 2017, 2018, 2019, 2020, 2021, 2022 and thereafter.\\nSpecifically, it states the estimated amortization expense for 2017 is $10509 (in thousands).\\nTherefore, the portion of the estimated amortization expense that will be recognized in 2017 is $10509.\\n</think>\\n$10509\\n<end_of_turn>']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02bf208-4c0f-431b-b2aa-11cf16d26b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8fa6f6-60be-480a-b9bd-7a6c07f2528e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b15b5f2b-5d0a-4dde-bf7f-5b39257150ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Please answer the given financial question based on the context.\n",
    "Context: incremental contract start-up costs 2014large municipal contract . during 2018 and 2017 , we incurred costs of $ 5.7 million and $ 8.2 million , respectively , related to the implementation of a large municipal contract . these costs did not meet the capitalization criteria prescribed by the new revenue recognition standard . adoption of the tax act . the tax act was enacted on december 22 , 2017 . among other things , the tax act reduced the u.s . federal corporate tax rate from 35% ( 35 % ) to 21% ( 21 % ) . for the year ended december 31 , 2017 , we recorded provisional amounts based on our estimates of the tax act 2019s effect to our deferred taxes , uncertain tax positions , and one-time transition tax . these adjustments reduced our tax provision by $ 463.9 million . during 2018 , we adjusted the provisional amounts recorded as of december 31 , 2017 for the one-time transition tax , deferred taxes and uncertain tax positions . these adjustments increased our tax provision by $ 0.3 million . bridgeton insurance recovery , net . during 2018 , we collected an insurance recovery of $ 40.0 million related to our closed bridgeton landfill in missouri , which we recognized as a reduction of remediation expenses in our cost of operations . in addition , we incurred $ 12.0 million of incremental costs attributable to the bridgeton insurance recovery . recent developments 2019 financial guidance in 2019 , we will continue to focus on managing the controllable aspects of our business by enhancing the quality of our revenue , investing in profitable growth opportunities and reducing costs . our team remains focused on executing our strategy to deliver consistent earnings and free cash flow growth , and improve return on invested capital . we are committed to an efficient capital structure , maintaining our investment grade credit ratings and increasing cash returned to our shareholders . our guidance is based on current economic conditions and does not assume any significant changes in the overall economy in 2019 . specific guidance follows : revenue we expect 2019 revenue to increase by approximately 4.25 to 4.75% ( 4.75 % ) comprised of the following : increase ( decrease ) .\n",
    "||increase ( decrease )|\n",
    "|average yield|2.75% ( 2.75 % )|\n",
    "|volume|0.0 to 0.25|\n",
    "|energy services|2013|\n",
    "|fuel recovery fees|0.25|\n",
    "|recycling processing and commodity sales|0.25 to 0.5|\n",
    "|acquisitions / divestitures net|1.0|\n",
    "|total change|4.25 to 4.75% ( 4.75 % )|\n",
    "changes in price are restricted on approximately 50% ( 50 % ) of our annual service revenue . the majority of these restricted pricing arrangements are tied to fluctuations in a specific index ( primarily a consumer price index ) as defined in the contract . the consumer price index varies from a single historical stated period of time or an average of trailing historical rates over a stated period of time . in addition , the initial effect of pricing resets typically lags 6 to 12 months from the end of the index measurement period to the date the revised pricing goes into effect . as a result , current changes in a specific index may not manifest themselves in our reported pricing for several quarters into the future. .\n",
    "Question: what is the ratio of the acquisitions / divestitures net to the fuel recovery fees as part of the expected 2019 revenue to increase\n",
    "Answer:\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b5da56b-4828-47d5-8c4e-57e23a7f5c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_format = prompt.format(question, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "072c840d-6c78-4644-8830-f6babb2e38f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([prompt_format], return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80ff6576-74f2-41bb-8215-6989017b64bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1024,\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9cb9cc18-042b-46a6-bba4-fd588a926731",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58ca9e5f-0ac3-409c-83cd-07f19879d45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos>There is a Question provided below. It has an instruction that describes a task, paired with the input that provides further context to the task.\\nWrite a response that appropriately completes the request.\\n\\nBefore providing the response, think about the question carefully and then create a step-by-step chain of thoughts to ensure that the response is logical and accurate.\\n\\n### Question:\\nPlease answer the given financial question based on the context.\\nContext: incremental contract start-up costs 2014large municipal contract . during 2018 and 2017 , we incurred costs of $ 5.7 million and $ 8.2 million , respectively , related to the implementation of a large municipal contract . these costs did not meet the capitalization criteria prescribed by the new revenue recognition standard . adoption of the tax act . the tax act was enacted on december 22 , 2017 . among other things , the tax act reduced the u.s . federal corporate tax rate from 35% ( 35 % ) to 21% ( 21 % ) . for the year ended december 31 , 2017 , we recorded provisional amounts based on our estimates of the tax act 2019s effect to our deferred taxes , uncertain tax positions , and one-time transition tax . these adjustments reduced our tax provision by $ 463.9 million . during 2018 , we adjusted the provisional amounts recorded as of december 31 , 2017 for the one-time transition tax , deferred taxes and uncertain tax positions . these adjustments increased our tax provision by $ 0.3 million . bridgeton insurance recovery , net . during 2018 , we collected an insurance recovery of $ 40.0 million related to our closed bridgeton landfill in missouri , which we recognized as a reduction of remediation expenses in our cost of operations . in addition , we incurred $ 12.0 million of incremental costs attributable to the bridgeton insurance recovery . recent developments 2019 financial guidance in 2019 , we will continue to focus on managing the controllable aspects of our business by enhancing the quality of our revenue , investing in profitable growth opportunities and reducing costs . our team remains focused on executing our strategy to deliver consistent earnings and free cash flow growth , and improve return on invested capital . we are committed to an efficient capital structure , maintaining our investment grade credit ratings and increasing cash returned to our shareholders . our guidance is based on current economic conditions and does not assume any significant changes in the overall economy in 2019 . specific guidance follows : revenue we expect 2019 revenue to increase by approximately 4.25 to 4.75% ( 4.75 % ) comprised of the following : increase ( decrease ) .\\n||increase ( decrease )|\\n|average yield|2.75% ( 2.75 % )|\\n|volume|0.0 to 0.25|\\n|energy services|2013|\\n|fuel recovery fees|0.25|\\n|recycling processing and commodity sales|0.25 to 0.5|\\n|acquisitions / divestitures net|1.0|\\n|total change|4.25 to 4.75% ( 4.75 % )|\\nchanges in price are restricted on approximately 50% ( 50 % ) of our annual service revenue . the majority of these restricted pricing arrangements are tied to fluctuations in a specific index ( primarily a consumer price index ) as defined in the contract . the consumer price index varies from a single historical stated period of time or an average of trailing historical rates over a stated period of time . in addition , the initial effect of pricing resets typically lags 6 to 12 months from the end of the index measurement period to the date the revised pricing goes into effect . as a result , current changes in a specific index may not manifest themselves in our reported pricing for several quarters into the future. .\\nQuestion: what is the ratio of the acquisitions / divestitures net to the fuel recovery fees as part of the expected 2019 revenue to increase\\nAnswer:\\n\\n### Response:\\n<think>\\nThe question asks for the ratio of \"acquisitions / divestitures net\" to \"fuel recovery fees\" in the expected 2019 revenue increase.\\n\\nFrom the provided context, we have:\\n*   \"acquisitions / divestitures net\" is expected to increase by 1.0.\\n*   \"fuel recovery fees\" is expected to increase by 0.25.\\n\\nThe ratio is (1.0) / (0.25) = 4.\\n</think>\\n4.  0\\n<end_of_turn>']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2016d5ce-010f-438b-a169-7778aa22bced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<think>\\nThe question asks for the ratio of \"acquisitions / divestitures net\" to \"fuel recovery fees\" in the expected 2019 revenue increase.\\n\\nFrom the provided context, we have:\\n*   \"acquisitions / divestitures net\" is expected to increase by 1.0.\\n*   \"fuel recovery fees\" is expected to increase by 0.25.\\n\\nThe ratio is (1.0) / (0.25) = 4.\\n</think>\\n4.  0\\n<end_of_turn>'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0].split(\"### Response:\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3543988-41a5-4f1a-ace3-10b2dab7341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(text_list):\n",
    "    extracted_data = []\n",
    "    for text in text_list:\n",
    "        # Find the start and end indices\n",
    "        start_idx = text.find('</think>\\n') + len('</think>\\n')\n",
    "        end_idx = text.find('\\n<end_of_turn>', start_idx)\n",
    "        \n",
    "        # Extract the text between the markers\n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            extracted = text[start_idx:end_idx].strip()\n",
    "            extracted_data.append(extracted)\n",
    "        else:\n",
    "            extracted_data.append(None)\n",
    "    \n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "976b3f97-ee67-4f2d-b9d1-5a7f30e0691d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4.  0']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"
     ]
    }
   ],
   "source": [
    "extract_data(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb60b34b-1de5-4780-835d-a0ff412440b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

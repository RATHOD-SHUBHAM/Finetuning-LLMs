{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44555124-5312-455d-be2a-250369ebad3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth\n",
      "  Downloading unsloth-2025.4.7-py3-none-any.whl.metadata (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting xformers\n",
      "  Downloading xformers-0.0.30-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting trl\n",
      "  Downloading trl-0.17.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.19.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting unsloth_zoo>=2025.4.4 (from unsloth)\n",
      "  Downloading unsloth_zoo-2025.4.4-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting triton>=3.0.0 (from unsloth)\n",
      "  Downloading triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth) (23.2)\n",
      "Collecting tyro (from unsloth)\n",
      "  Downloading tyro-0.9.19-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting sentencepiece>=0.2.0 (from unsloth)\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tqdm (from unsloth)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth) (5.9.6)\n",
      "Collecting wheel>=0.42.0 (from unsloth)\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.24.1)\n",
      "Collecting accelerate>=0.34.1 (from unsloth)\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting trl\n",
      "  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting protobuf<4.0.0 (from unsloth)\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Collecting hf_transfer (from unsloth)\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting diffusers (from unsloth)\n",
      "  Downloading diffusers-0.33.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests (from transformers)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2023.4.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton>=3.0.0->unsloth) (68.2.2)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.3.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting rich (from trl)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting click!=8.0.0,>=7.1 (from wandb)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (3.11.0)\n",
      "Collecting pydantic<3 (from wandb)\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.27.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "INFO: pip is looking at multiple versions of fsspec[http] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3->wandb)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3->wandb)\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3->wandb)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.4.4->unsloth)\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting msgspec (from unsloth_zoo>=2025.4.4->unsloth)\n",
      "  Downloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from diffusers->unsloth) (4.6.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->trl)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.16.1)\n",
      "Collecting docstring-parser>=0.15 (from tyro->unsloth)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
      "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro->unsloth)\n",
      "  Downloading typeguard-4.4.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "\u001b[33mWARNING: huggingface-hub 0.30.2 does not provide the extra 'hf-xet'\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->trl)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading unsloth-2025.4.7-py3-none-any.whl (218 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.5/218.5 kB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m273.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m164.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.30-cp310-cp310-manylinux_2_28_x86_64.whl (31.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m194.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl (865.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m216.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m199.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m220.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m156.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m196.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.4/156.4 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.4/481.4 kB\u001b[0m \u001b[31m170.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.22.0-cp310-cp310-manylinux_2_28_x86_64.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m257.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m253.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m129.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m148.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m137.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m212.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.19.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m214.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m140.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m244.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m218.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m168.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.9/443.9 kB\u001b[0m \u001b[31m152.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m242.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m200.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m171.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m222.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.27.0-py2.py3-none-any.whl (340 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.8/340.8 kB\u001b[0m \u001b[31m147.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m269.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m260.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unsloth_zoo-2025.4.4-py3-none-any.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading diffusers-0.33.1-py3-none-any.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m228.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m262.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m253.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading tyro-0.9.19-py3-none-any.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.3/287.3 kB\u001b[0m \u001b[31m136.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.8/219.8 kB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m185.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Downloading typeguard-4.4.2-py3-none-any.whl (35 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m156.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.9/333.9 kB\u001b[0m \u001b[31m151.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.6/211.6 kB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: sentencepiece, pytz, nvidia-cusparselt-cu12, xxhash, wheel, tzdata, typing-extensions, triton, tqdm, sympy, smmap, shtab, setproctitle, sentry-sdk, safetensors, requests, regex, pyarrow, protobuf, propcache, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, mdurl, hf_transfer, fsspec, frozenlist, docstring-parser, docker-pycreds, dill, click, async-timeout, annotated-types, aiohappyeyeballs, typing-inspection, typeguard, tiktoken, pydantic-core, pandas, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, multiprocess, multidict, markdown-it-py, huggingface_hub, gitdb, aiosignal, yarl, tokenizers, rich, pydantic, nvidia-cusolver-cu12, gitpython, diffusers, wandb, tyro, transformers, torch, aiohttp, xformers, torchvision, torchaudio, cut_cross_entropy, bitsandbytes, accelerate, peft, datasets, trl, unsloth_zoo, unsloth\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.41.3\n",
      "    Uninstalling wheel-0.41.3:\n",
      "      Successfully uninstalled wheel-0.41.3\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0+cu118\n",
      "    Uninstalling torch-2.1.0+cu118:\n",
      "      Successfully uninstalled torch-2.1.0+cu118\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.16.0+cu118\n",
      "    Uninstalling torchvision-0.16.0+cu118:\n",
      "      Successfully uninstalled torchvision-0.16.0+cu118\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.1.0+cu118\n",
      "    Uninstalling torchaudio-2.1.0+cu118:\n",
      "      Successfully uninstalled torchaudio-2.1.0+cu118\n",
      "Successfully installed accelerate-1.6.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 annotated-types-0.7.0 async-timeout-5.0.1 bitsandbytes-0.45.5 click-8.1.8 cut_cross_entropy-25.1.1 datasets-3.5.1 diffusers-0.33.1 dill-0.3.8 docker-pycreds-0.4.0 docstring-parser-0.16 frozenlist-1.6.0 fsspec-2025.3.0 gitdb-4.0.12 gitpython-3.1.44 hf_transfer-0.1.9 huggingface_hub-0.30.2 markdown-it-py-3.0.0 mdurl-0.1.2 msgspec-0.19.0 multidict-6.4.3 multiprocess-0.70.16 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 pandas-2.2.3 peft-0.15.2 propcache-0.3.1 protobuf-3.20.3 pyarrow-20.0.0 pydantic-2.11.4 pydantic-core-2.33.2 pytz-2025.2 regex-2024.11.6 requests-2.32.3 rich-14.0.0 safetensors-0.5.3 sentencepiece-0.2.0 sentry-sdk-2.27.0 setproctitle-1.3.6 shtab-1.7.2 smmap-5.0.2 sympy-1.14.0 tiktoken-0.9.0 tokenizers-0.21.1 torch-2.7.0 torchaudio-2.7.0 torchvision-0.22.0 tqdm-4.67.1 transformers-4.51.3 triton-3.3.0 trl-0.15.2 typeguard-4.4.2 typing-extensions-4.13.2 typing-inspection-0.4.0 tyro-0.9.19 tzdata-2025.2 unsloth-2025.4.7 unsloth_zoo-2025.4.4 wandb-0.19.10 wheel-0.45.1 xformers-0.0.30 xxhash-3.5.0 yarl-1.20.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install unsloth transformers datasets xformers huggingface_hub torch torchvision torchaudio bitsandbytes peft trl tiktoken wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe63229-654d-4e13-b511-7365ce2ff8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "699e7aa3-5ec8-4b2b-b122-f82c2f46b831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2af3ec36db444b4af4216a947be3022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a33859-e2b8-4dc8-b21b-f51d7ac9b626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa422fd-0b74-407e-a78b-6ae00dc890e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6b17eaf-39b3-4ee6-8a73-09b20f7f7a71",
   "metadata": {},
   "source": [
    "# WandB\n",
    "\n",
    "init(): https://docs.wandb.ai/ref/python/init/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "844e6e3a-6d79-4aac-b52c-5b4c1ed44e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlaravelshubham\u001b[0m (\u001b[33mlaravelshubham-student\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aaec088-535d-45bd-9bb8-a43b049f148e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/Ft_Gemma/wandb/run-20250504_190702-m2grhgzs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/laravelshubham-student/Fine-tuning%20Gemma-3-4B%20on%20FinQA%20Reasoning%20Dataset/runs/m2grhgzs?apiKey=9cec55360d515cebf3a98e5f8de569b5c44f6265' target=\"_blank\">rebel-admiral-1</a></strong> to <a href='https://wandb.ai/laravelshubham-student/Fine-tuning%20Gemma-3-4B%20on%20FinQA%20Reasoning%20Dataset?apiKey=9cec55360d515cebf3a98e5f8de569b5c44f6265' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/laravelshubham-student/Fine-tuning%20Gemma-3-4B%20on%20FinQA%20Reasoning%20Dataset?apiKey=9cec55360d515cebf3a98e5f8de569b5c44f6265' target=\"_blank\">https://wandb.ai/laravelshubham-student/Fine-tuning%20Gemma-3-4B%20on%20FinQA%20Reasoning%20Dataset?apiKey=9cec55360d515cebf3a98e5f8de569b5c44f6265</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/laravelshubham-student/Fine-tuning%20Gemma-3-4B%20on%20FinQA%20Reasoning%20Dataset/runs/m2grhgzs?apiKey=9cec55360d515cebf3a98e5f8de569b5c44f6265' target=\"_blank\">https://wandb.ai/laravelshubham-student/Fine-tuning%20Gemma-3-4B%20on%20FinQA%20Reasoning%20Dataset/runs/m2grhgzs?apiKey=9cec55360d515cebf3a98e5f8de569b5c44f6265</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Do NOT share these links with anyone. They can be used to claim your runs."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project='Fine-tuning Gemma-3-4B on FinQA Reasoning Dataset', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6e017c-b854-4eb8-b2e4-55e29e52366a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f18dca-4f5d-491e-89e8-5dd40f6ee7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ea9cc17-798e-4f98-b107-15836c16a9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: We'll be using `/tmp/unsloth_compiled_cache` for temporary Unsloth patches.\n",
      "Standard import failed for UnslothGKDTrainer: No module named 'UnslothGKDTrainer'. Using tempfile instead!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a9143f8-f982-4e5d-b49e-72ae86225efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.7: Fast Gemma3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA RTX 6000 Ada Generation. Num GPUs = 1. Max memory: 47.5 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02322677eeff4bd0b10455e98f3db5ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bfc9686ab3f4342bef0f8de6386f8a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23de63713f147d2bbdb692259bcc410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d04e611f6142a4971bc32f77e03262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb3fd7e52ef48f9b5df215fa5868516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4a2cd6c6d745e0a54829677dd50ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b6e8c50ede47338f6f0993c04b0c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d93df3336bd4289a2332e536ec765c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e8b96e3dde4c4ba47e6b49ceb90d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993293cdda4246d0a563242e91d95e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149f862c469148e0a49627d7ae498c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it\",\n",
    "    max_seq_length = 2048, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dfc7e6-4e29-4f75-a031-cf00437ffd87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e5f809e-69e1-4d5d-a624-522fb8a3b583",
   "metadata": {},
   "source": [
    "### Now add LoRA adapters so we only need to update a small amount of parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3cad224-8dbb-45be-8297-d608161867b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.language_model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "\n",
    "    r = 8,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 8,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff4cb5b-0541-4f2f-80a1-085cbbaf570f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b47b4b29-e35c-498d-a9a9-7f6f60c5ada2",
   "metadata": {},
   "source": [
    "## System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05ae9bc2-c0b5-49bb-8dce-091f8bd753fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "There is a Question provided below. It has an instruction that describes a task, paired with the input that provides further context to the task.\n",
    "Write a response that appropriately completes the request.\n",
    "\n",
    "Before providing the response, think about the question carefully and then create a step-by-step chain of thoughts to ensure that the response is logical and accurate.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "{}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abecfa4-0752-49de-9ad1-df1dabbbb38c",
   "metadata": {},
   "source": [
    "#### Why to add EOS\n",
    "\n",
    "Yes, for models like Gemma (and other decoder-only models like LLaMA, Falcon, Mistral), it’s essential to add the End-of-Sequence (EOS) token explicitly in certain scenarios — especially when training, fine-tuning, or prompting for generation.\n",
    "\n",
    "🧠 Why You Must Add the EOS Token in Gemma\n",
    "1️⃣ Gemma is a decoder-only causal language model\n",
    "It learns to predict the next token, one token at a time, based on previous context. It needs to know when a sequence ends — and that’s what the eos_token_id marks.\n",
    "\n",
    "2️⃣ During training and generation, EOS serves two roles:\n",
    "Purpose\tEffect\n",
    "In training\tTells the model that prediction should stop at a certain point, helps it learn boundaries between prompt and response.\n",
    "In generation\tModel will stop decoding when it outputs the eos_token_id, saving time and preventing infinite loops.\n",
    "\n",
    "⚙️ How to Know When You Must Add EOS\n",
    "There are 3 clear signs:\n",
    "\n",
    "✅ 1. Tokenizer config lacks automatic EOS addition\n",
    "\n",
    "✅ 2. Gemma model docs say it\n",
    "In the Gemma docs, it clearly states:\n",
    "\n",
    "❗️ \"To get the model to stop generating, you must ensure the input includes the eos_token_id, as the model was trained to stop generation upon seeing it.\"\n",
    "\n",
    "✅ 3. generate() doesn’t stop unless EOS is reached\n",
    "If your .generate() keeps going too long or hits the max_new_tokens limit every time, it's a sign that:\n",
    "\n",
    "The model doesn’t know when to stop\n",
    "\n",
    "It never emitted eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "896cab42-902a-4f2e-bc9c-4dc733e011b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa441bfc-1199-4c0a-b6f1-95a877e77207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_formatting_prompts_func(examples):\n",
    "    inputs = examples[\"Open-ended Verifiable Question\"]\n",
    "    complex_cots = examples[\"Complex_CoT\"]\n",
    "    outputs = examples[\"Response\"]\n",
    "    texts = []\n",
    "\n",
    "    zipped = zip(inputs,complex_cots,outputs)\n",
    "    print(\"Check zipped: \", zipped)\n",
    "    print(\"=======================\")\n",
    "    \n",
    "    # For each triplet (input, cot, output), it formats them into a single string using system_prompt.format().\n",
    "    for input, cot, output in zip(inputs,complex_cots,outputs):\n",
    "        print(\"zipped:  \", input,cot,output)\n",
    "        print(\"=======================\")\n",
    "        text = system_prompt.format(input,cot,output) + EOS_TOKEN\n",
    "        print(\"Formatted Value: \",texts)\n",
    "        print(\"=======================\")\n",
    "        texts.append(text)\n",
    "        print(\"========DONE===============\")\n",
    "    \n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e16eb25-6b4e-487b-abd0-73e3a67de929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"Open-ended Verifiable Question\"]\n",
    "    complex_cots = examples[\"Complex_CoT\"]\n",
    "    outputs = examples[\"Response\"]\n",
    "    texts = []\n",
    "\n",
    "    \n",
    "    # For each triplet (input, cot, output), it formats them into a single string using system_prompt.format().\n",
    "    for input, cot, output in zip(inputs,complex_cots,outputs):\n",
    "        text = system_prompt.format(input,cot,output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "        \n",
    "    \n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0539e3fc-ba8a-4a9b-bbed-c55e62b6ccec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a42ab93a-e88e-4242-a82b-23f547678113",
   "metadata": {},
   "source": [
    "## Pull and Format the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0f1c1e0-ff09-4480-8995-f4d4dfa8215b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda5f31a85b14d30965367aee49bca57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b2e26bf7694fcfbfa1dd55183deec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.parquet:   0%|          | 0.00/14.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949f9662f0b742c4abb980cd1780e8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"TheFinAI/Fino1_Reasoning_Path_FinQA\", split = \"train[0:500]\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccabb825-f142-4a75-819d-adf03507e49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a2ae72f-a34c-4688-8b78-b751dd139c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Open-ended Verifiable Question': 'Please answer the given financial question based on the context.\\nContext: amortization expense , which is included in selling , general and administrative expenses , was $ 13.0 million , $ 13.9 million and $ 8.5 million for the years ended december 31 , 2016 , 2015 and 2014 , respectively . the following is the estimated amortization expense for the company 2019s intangible assets as of december 31 , 2016 : ( in thousands ) .\\n|2017|$ 10509|\\n|2018|9346|\\n|2019|9240|\\n|2020|7201|\\n|2021|5318|\\n|2022 and thereafter|16756|\\n|amortization expense of intangible assets|$ 58370|\\nat december 31 , 2016 , 2015 and 2014 , the company determined that its goodwill and indefinite- lived intangible assets were not impaired . 5 . credit facility and other long term debt credit facility the company is party to a credit agreement that provides revolving commitments for up to $ 1.25 billion of borrowings , as well as term loan commitments , in each case maturing in january 2021 . as of december 31 , 2016 there was no outstanding balance under the revolving credit facility and $ 186.3 million of term loan borrowings remained outstanding . at the company 2019s request and the lender 2019s consent , revolving and or term loan borrowings may be increased by up to $ 300.0 million in aggregate , subject to certain conditions as set forth in the credit agreement , as amended . incremental borrowings are uncommitted and the availability thereof , will depend on market conditions at the time the company seeks to incur such borrowings . the borrowings under the revolving credit facility have maturities of less than one year . up to $ 50.0 million of the facility may be used for the issuance of letters of credit . there were $ 2.6 million of letters of credit outstanding as of december 31 , 2016 . the credit agreement contains negative covenants that , subject to significant exceptions , limit the ability of the company and its subsidiaries to , among other things , incur additional indebtedness , make restricted payments , pledge their assets as security , make investments , loans , advances , guarantees and acquisitions , undergo fundamental changes and enter into transactions with affiliates . the company is also required to maintain a ratio of consolidated ebitda , as defined in the credit agreement , to consolidated interest expense of not less than 3.50 to 1.00 and is not permitted to allow the ratio of consolidated total indebtedness to consolidated ebitda to be greater than 3.25 to 1.00 ( 201cconsolidated leverage ratio 201d ) . as of december 31 , 2016 , the company was in compliance with these ratios . in addition , the credit agreement contains events of default that are customary for a facility of this nature , and includes a cross default provision whereby an event of default under other material indebtedness , as defined in the credit agreement , will be considered an event of default under the credit agreement . borrowings under the credit agreement bear interest at a rate per annum equal to , at the company 2019s option , either ( a ) an alternate base rate , or ( b ) a rate based on the rates applicable for deposits in the interbank market for u.s . dollars or the applicable currency in which the loans are made ( 201cadjusted libor 201d ) , plus in each case an applicable margin . the applicable margin for loans will .\\nQuestion: what portion of the estimated amortization expense will be recognized in 2017?\\nAnswer:',\n",
       " 'Ground-True Answer': '0.18004',\n",
       " 'Complex_CoT': \"Alright, let's dive into this question about amortization expenses. First, I need to identify the table that illustrates the estimated amortization expenses from 2017 onwards. It looks like this table is super important in figuring out the expense for 2017. \\n\\nOkay, found it! The table clearly states that the estimated amortization expense for the year 2017 is $10,509, but it notes that this is in thousands. \\n\\nLet me think for a second. If the numbers are in thousands, that means I need to convert that $10,509 into actual dollars, which becomes $10,509,000. Easy enough, I just multiply by a thousand. \\n\\nI need to make sure this is consistent throughout. Let me double-check if there's any other information that throws a wrench in this conversion... Nope, everything seems straightforward here. \\n\\nSo, what's the conclusion? The amortization expense for 2017 is a clear-cut $10,509,000. Hmm, that makes sense based on the conversion from thousands of dollars, and it looks like there aren't any surprise twists or exceptions here. \\n\\nBut hold up, let me consider the perspective of portions. The context refers to percentages or parts, and for that, I should be thinking about the total amortization expense. The whole amount listed is $58,370,000 for all the intangible assets. \\n\\nTo find out what part of the total this 2017 expense represents, I'll calculate a ratio. So, I divide $10,509,000 by the grand total, $58,370,000. Crunching these numbers gives me around 0.18004.\\n\\nSo, in conclusion, the portion of the total estimated amortization expense that is specifically for 2017 is approximately 0.18004 of the total. I'm confident that's correct since that ratio aligns perfectly with the expectation of finding a portion or part. \\n\\nPhew, that should do it!\",\n",
       " 'Response': 'To determine the portion of the estimated amortization expense that will be recognized in 2017, we first examine the table provided. The estimated amortization expense for 2017 is $10,509,000 (noting that the figures are in thousands, hence $10,509 becomes $10,509,000). \\n\\nThe total estimated amortization expense for all the listed years is $58,370,000. To find out what portion of the total this represents for 2017, we take the 2017 expense and divide it by the total:\\n\\n\\\\[ \\\\text{Portion for 2017} = \\\\frac{\\\\text{2017 expense}}{\\\\text{Total expense}} = \\\\frac{10,509,000}{58,370,000} \\\\approx 0.18004 \\\\]\\n\\nTherefore, approximately 18.004% of the total estimated amortization expense will be recognized in 2017.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ip = dataset[0]\n",
    "test_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fea5c2c2-b6c4-4745-b796-acf9cf12df58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47158763-6922-431a-8388-5825fa30d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_result = test_formatting_prompts_func(test_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedbf14f-2c6e-4348-91b2-5d6a08faae03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ecba3f6-d24d-4436-8952-e2ba0f9a5e2e",
   "metadata": {},
   "source": [
    "### Map dataset in batches\n",
    "\n",
    "The dataset.map() function applies formatting_prompts_func to all batches in the dataset.\n",
    "\n",
    "The original dataset is transformed into a new dataset where each example is replaced by the formatted text.\n",
    "\n",
    "The new dataset will have a column named \"text\" containing the formatted prompts (instead of the original separate columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5672c9ce-1094-4aa9-ac42-521394936c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4325ea8626904953993398317b6bf06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a61bcead-f2cc-4efc-a7d2-9302d209ee5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Open-ended Verifiable Question': 'Please answer the given financial question based on the context.\\nContext: amortization expense , which is included in selling , general and administrative expenses , was $ 13.0 million , $ 13.9 million and $ 8.5 million for the years ended december 31 , 2016 , 2015 and 2014 , respectively . the following is the estimated amortization expense for the company 2019s intangible assets as of december 31 , 2016 : ( in thousands ) .\\n|2017|$ 10509|\\n|2018|9346|\\n|2019|9240|\\n|2020|7201|\\n|2021|5318|\\n|2022 and thereafter|16756|\\n|amortization expense of intangible assets|$ 58370|\\nat december 31 , 2016 , 2015 and 2014 , the company determined that its goodwill and indefinite- lived intangible assets were not impaired . 5 . credit facility and other long term debt credit facility the company is party to a credit agreement that provides revolving commitments for up to $ 1.25 billion of borrowings , as well as term loan commitments , in each case maturing in january 2021 . as of december 31 , 2016 there was no outstanding balance under the revolving credit facility and $ 186.3 million of term loan borrowings remained outstanding . at the company 2019s request and the lender 2019s consent , revolving and or term loan borrowings may be increased by up to $ 300.0 million in aggregate , subject to certain conditions as set forth in the credit agreement , as amended . incremental borrowings are uncommitted and the availability thereof , will depend on market conditions at the time the company seeks to incur such borrowings . the borrowings under the revolving credit facility have maturities of less than one year . up to $ 50.0 million of the facility may be used for the issuance of letters of credit . there were $ 2.6 million of letters of credit outstanding as of december 31 , 2016 . the credit agreement contains negative covenants that , subject to significant exceptions , limit the ability of the company and its subsidiaries to , among other things , incur additional indebtedness , make restricted payments , pledge their assets as security , make investments , loans , advances , guarantees and acquisitions , undergo fundamental changes and enter into transactions with affiliates . the company is also required to maintain a ratio of consolidated ebitda , as defined in the credit agreement , to consolidated interest expense of not less than 3.50 to 1.00 and is not permitted to allow the ratio of consolidated total indebtedness to consolidated ebitda to be greater than 3.25 to 1.00 ( 201cconsolidated leverage ratio 201d ) . as of december 31 , 2016 , the company was in compliance with these ratios . in addition , the credit agreement contains events of default that are customary for a facility of this nature , and includes a cross default provision whereby an event of default under other material indebtedness , as defined in the credit agreement , will be considered an event of default under the credit agreement . borrowings under the credit agreement bear interest at a rate per annum equal to , at the company 2019s option , either ( a ) an alternate base rate , or ( b ) a rate based on the rates applicable for deposits in the interbank market for u.s . dollars or the applicable currency in which the loans are made ( 201cadjusted libor 201d ) , plus in each case an applicable margin . the applicable margin for loans will .\\nQuestion: what portion of the estimated amortization expense will be recognized in 2017?\\nAnswer:',\n",
       " 'Ground-True Answer': '0.18004',\n",
       " 'Complex_CoT': \"Alright, let's dive into this question about amortization expenses. First, I need to identify the table that illustrates the estimated amortization expenses from 2017 onwards. It looks like this table is super important in figuring out the expense for 2017. \\n\\nOkay, found it! The table clearly states that the estimated amortization expense for the year 2017 is $10,509, but it notes that this is in thousands. \\n\\nLet me think for a second. If the numbers are in thousands, that means I need to convert that $10,509 into actual dollars, which becomes $10,509,000. Easy enough, I just multiply by a thousand. \\n\\nI need to make sure this is consistent throughout. Let me double-check if there's any other information that throws a wrench in this conversion... Nope, everything seems straightforward here. \\n\\nSo, what's the conclusion? The amortization expense for 2017 is a clear-cut $10,509,000. Hmm, that makes sense based on the conversion from thousands of dollars, and it looks like there aren't any surprise twists or exceptions here. \\n\\nBut hold up, let me consider the perspective of portions. The context refers to percentages or parts, and for that, I should be thinking about the total amortization expense. The whole amount listed is $58,370,000 for all the intangible assets. \\n\\nTo find out what part of the total this 2017 expense represents, I'll calculate a ratio. So, I divide $10,509,000 by the grand total, $58,370,000. Crunching these numbers gives me around 0.18004.\\n\\nSo, in conclusion, the portion of the total estimated amortization expense that is specifically for 2017 is approximately 0.18004 of the total. I'm confident that's correct since that ratio aligns perfectly with the expectation of finding a portion or part. \\n\\nPhew, that should do it!\",\n",
       " 'Response': 'To determine the portion of the estimated amortization expense that will be recognized in 2017, we first examine the table provided. The estimated amortization expense for 2017 is $10,509,000 (noting that the figures are in thousands, hence $10,509 becomes $10,509,000). \\n\\nThe total estimated amortization expense for all the listed years is $58,370,000. To find out what portion of the total this represents for 2017, we take the 2017 expense and divide it by the total:\\n\\n\\\\[ \\\\text{Portion for 2017} = \\\\frac{\\\\text{2017 expense}}{\\\\text{Total expense}} = \\\\frac{10,509,000}{58,370,000} \\\\approx 0.18004 \\\\]\\n\\nTherefore, approximately 18.004% of the total estimated amortization expense will be recognized in 2017.',\n",
       " 'text': \"There is a Question provided below. It has an instruction that describes a task, paired with the input that provides further context to the task.\\nWrite a response that appropriately completes the request.\\n\\nBefore providing the response, think about the question carefully and then create a step-by-step chain of thoughts to ensure that the response is logical and accurate.\\n\\n### Question:\\nPlease answer the given financial question based on the context.\\nContext: amortization expense , which is included in selling , general and administrative expenses , was $ 13.0 million , $ 13.9 million and $ 8.5 million for the years ended december 31 , 2016 , 2015 and 2014 , respectively . the following is the estimated amortization expense for the company 2019s intangible assets as of december 31 , 2016 : ( in thousands ) .\\n|2017|$ 10509|\\n|2018|9346|\\n|2019|9240|\\n|2020|7201|\\n|2021|5318|\\n|2022 and thereafter|16756|\\n|amortization expense of intangible assets|$ 58370|\\nat december 31 , 2016 , 2015 and 2014 , the company determined that its goodwill and indefinite- lived intangible assets were not impaired . 5 . credit facility and other long term debt credit facility the company is party to a credit agreement that provides revolving commitments for up to $ 1.25 billion of borrowings , as well as term loan commitments , in each case maturing in january 2021 . as of december 31 , 2016 there was no outstanding balance under the revolving credit facility and $ 186.3 million of term loan borrowings remained outstanding . at the company 2019s request and the lender 2019s consent , revolving and or term loan borrowings may be increased by up to $ 300.0 million in aggregate , subject to certain conditions as set forth in the credit agreement , as amended . incremental borrowings are uncommitted and the availability thereof , will depend on market conditions at the time the company seeks to incur such borrowings . the borrowings under the revolving credit facility have maturities of less than one year . up to $ 50.0 million of the facility may be used for the issuance of letters of credit . there were $ 2.6 million of letters of credit outstanding as of december 31 , 2016 . the credit agreement contains negative covenants that , subject to significant exceptions , limit the ability of the company and its subsidiaries to , among other things , incur additional indebtedness , make restricted payments , pledge their assets as security , make investments , loans , advances , guarantees and acquisitions , undergo fundamental changes and enter into transactions with affiliates . the company is also required to maintain a ratio of consolidated ebitda , as defined in the credit agreement , to consolidated interest expense of not less than 3.50 to 1.00 and is not permitted to allow the ratio of consolidated total indebtedness to consolidated ebitda to be greater than 3.25 to 1.00 ( 201cconsolidated leverage ratio 201d ) . as of december 31 , 2016 , the company was in compliance with these ratios . in addition , the credit agreement contains events of default that are customary for a facility of this nature , and includes a cross default provision whereby an event of default under other material indebtedness , as defined in the credit agreement , will be considered an event of default under the credit agreement . borrowings under the credit agreement bear interest at a rate per annum equal to , at the company 2019s option , either ( a ) an alternate base rate , or ( b ) a rate based on the rates applicable for deposits in the interbank market for u.s . dollars or the applicable currency in which the loans are made ( 201cadjusted libor 201d ) , plus in each case an applicable margin . the applicable margin for loans will .\\nQuestion: what portion of the estimated amortization expense will be recognized in 2017?\\nAnswer:\\n\\n### Response:\\n<think>\\nAlright, let's dive into this question about amortization expenses. First, I need to identify the table that illustrates the estimated amortization expenses from 2017 onwards. It looks like this table is super important in figuring out the expense for 2017. \\n\\nOkay, found it! The table clearly states that the estimated amortization expense for the year 2017 is $10,509, but it notes that this is in thousands. \\n\\nLet me think for a second. If the numbers are in thousands, that means I need to convert that $10,509 into actual dollars, which becomes $10,509,000. Easy enough, I just multiply by a thousand. \\n\\nI need to make sure this is consistent throughout. Let me double-check if there's any other information that throws a wrench in this conversion... Nope, everything seems straightforward here. \\n\\nSo, what's the conclusion? The amortization expense for 2017 is a clear-cut $10,509,000. Hmm, that makes sense based on the conversion from thousands of dollars, and it looks like there aren't any surprise twists or exceptions here. \\n\\nBut hold up, let me consider the perspective of portions. The context refers to percentages or parts, and for that, I should be thinking about the total amortization expense. The whole amount listed is $58,370,000 for all the intangible assets. \\n\\nTo find out what part of the total this 2017 expense represents, I'll calculate a ratio. So, I divide $10,509,000 by the grand total, $58,370,000. Crunching these numbers gives me around 0.18004.\\n\\nSo, in conclusion, the portion of the total estimated amortization expense that is specifically for 2017 is approximately 0.18004 of the total. I'm confident that's correct since that ratio aligns perfectly with the expectation of finding a portion or part. \\n\\nPhew, that should do it!\\n</think>\\nTo determine the portion of the estimated amortization expense that will be recognized in 2017, we first examine the table provided. The estimated amortization expense for 2017 is $10,509,000 (noting that the figures are in thousands, hence $10,509 becomes $10,509,000). \\n\\nThe total estimated amortization expense for all the listed years is $58,370,000. To find out what portion of the total this represents for 2017, we take the 2017 expense and divide it by the total:\\n\\n\\\\[ \\\\text{Portion for 2017} = \\\\frac{\\\\text{2017 expense}}{\\\\text{Total expense}} = \\\\frac{10,509,000}{58,370,000} \\\\approx 0.18004 \\\\]\\n\\nTherefore, approximately 18.004% of the total estimated amortization expense will be recognized in 2017.<end_of_turn>\"}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4451ee5-e18e-4acd-9881-42c84e6f940f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There is a Question provided below. It has an instruction that describes a task, paired with the input that provides further context to the task.\\nWrite a response that appropriately completes the request.\\n\\nBefore providing the response, think about the question carefully and then create a step-by-step chain of thoughts to ensure that the response is logical and accurate.\\n\\n### Question:\\nPlease answer the given financial question based on the context.\\nContext: amortization expense , which is included in selling , general and administrative expenses , was $ 13.0 million , $ 13.9 million and $ 8.5 million for the years ended december 31 , 2016 , 2015 and 2014 , respectively . the following is the estimated amortization expense for the company 2019s intangible assets as of december 31 , 2016 : ( in thousands ) .\\n|2017|$ 10509|\\n|2018|9346|\\n|2019|9240|\\n|2020|7201|\\n|2021|5318|\\n|2022 and thereafter|16756|\\n|amortization expense of intangible assets|$ 58370|\\nat december 31 , 2016 , 2015 and 2014 , the company determined that its goodwill and indefinite- lived intangible assets were not impaired . 5 . credit facility and other long term debt credit facility the company is party to a credit agreement that provides revolving commitments for up to $ 1.25 billion of borrowings , as well as term loan commitments , in each case maturing in january 2021 . as of december 31 , 2016 there was no outstanding balance under the revolving credit facility and $ 186.3 million of term loan borrowings remained outstanding . at the company 2019s request and the lender 2019s consent , revolving and or term loan borrowings may be increased by up to $ 300.0 million in aggregate , subject to certain conditions as set forth in the credit agreement , as amended . incremental borrowings are uncommitted and the availability thereof , will depend on market conditions at the time the company seeks to incur such borrowings . the borrowings under the revolving credit facility have maturities of less than one year . up to $ 50.0 million of the facility may be used for the issuance of letters of credit . there were $ 2.6 million of letters of credit outstanding as of december 31 , 2016 . the credit agreement contains negative covenants that , subject to significant exceptions , limit the ability of the company and its subsidiaries to , among other things , incur additional indebtedness , make restricted payments , pledge their assets as security , make investments , loans , advances , guarantees and acquisitions , undergo fundamental changes and enter into transactions with affiliates . the company is also required to maintain a ratio of consolidated ebitda , as defined in the credit agreement , to consolidated interest expense of not less than 3.50 to 1.00 and is not permitted to allow the ratio of consolidated total indebtedness to consolidated ebitda to be greater than 3.25 to 1.00 ( 201cconsolidated leverage ratio 201d ) . as of december 31 , 2016 , the company was in compliance with these ratios . in addition , the credit agreement contains events of default that are customary for a facility of this nature , and includes a cross default provision whereby an event of default under other material indebtedness , as defined in the credit agreement , will be considered an event of default under the credit agreement . borrowings under the credit agreement bear interest at a rate per annum equal to , at the company 2019s option , either ( a ) an alternate base rate , or ( b ) a rate based on the rates applicable for deposits in the interbank market for u.s . dollars or the applicable currency in which the loans are made ( 201cadjusted libor 201d ) , plus in each case an applicable margin . the applicable margin for loans will .\\nQuestion: what portion of the estimated amortization expense will be recognized in 2017?\\nAnswer:\\n\\n### Response:\\n<think>\\nAlright, let's dive into this question about amortization expenses. First, I need to identify the table that illustrates the estimated amortization expenses from 2017 onwards. It looks like this table is super important in figuring out the expense for 2017. \\n\\nOkay, found it! The table clearly states that the estimated amortization expense for the year 2017 is $10,509, but it notes that this is in thousands. \\n\\nLet me think for a second. If the numbers are in thousands, that means I need to convert that $10,509 into actual dollars, which becomes $10,509,000. Easy enough, I just multiply by a thousand. \\n\\nI need to make sure this is consistent throughout. Let me double-check if there's any other information that throws a wrench in this conversion... Nope, everything seems straightforward here. \\n\\nSo, what's the conclusion? The amortization expense for 2017 is a clear-cut $10,509,000. Hmm, that makes sense based on the conversion from thousands of dollars, and it looks like there aren't any surprise twists or exceptions here. \\n\\nBut hold up, let me consider the perspective of portions. The context refers to percentages or parts, and for that, I should be thinking about the total amortization expense. The whole amount listed is $58,370,000 for all the intangible assets. \\n\\nTo find out what part of the total this 2017 expense represents, I'll calculate a ratio. So, I divide $10,509,000 by the grand total, $58,370,000. Crunching these numbers gives me around 0.18004.\\n\\nSo, in conclusion, the portion of the total estimated amortization expense that is specifically for 2017 is approximately 0.18004 of the total. I'm confident that's correct since that ratio aligns perfectly with the expectation of finding a portion or part. \\n\\nPhew, that should do it!\\n</think>\\nTo determine the portion of the estimated amortization expense that will be recognized in 2017, we first examine the table provided. The estimated amortization expense for 2017 is $10,509,000 (noting that the figures are in thousands, hence $10,509 becomes $10,509,000). \\n\\nThe total estimated amortization expense for all the listed years is $58,370,000. To find out what portion of the total this represents for 2017, we take the 2017 expense and divide it by the total:\\n\\n\\\\[ \\\\text{Portion for 2017} = \\\\frac{\\\\text{2017 expense}}{\\\\text{Total expense}} = \\\\frac{10,509,000}{58,370,000} \\\\approx 0.18004 \\\\]\\n\\nTherefore, approximately 18.004% of the total estimated amortization expense will be recognized in 2017.<end_of_turn>\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f04edc5-7a11-46cb-b485-dad02f0ead1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbea7a58-3fa9-4759-afd4-eb7163f70855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cd8658-69f5-44f7-b3c0-8d7b419aa857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21d31041-8dcf-45df-aff1-acd23428d14e",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95066325-1d9b-4edd-9e76-7e9fd8df5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# trainer = SFTTrainer(\n",
    "#     model = model,\n",
    "#     tokenizer = tokenizer,\n",
    "#     train_dataset = dataset,\n",
    "    \n",
    "#     args = SFTConfig(\n",
    "#         output_dir=\"outputs\",\n",
    "#         dataset_text_field = \"text\",\n",
    "#         per_device_train_batch_size = 2,\n",
    "#         gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "#         warmup_steps = 5,\n",
    "#         # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "#         max_steps = 30,\n",
    "#         max_seq_length=1024,\n",
    "#         learning_rate = 2e-3, # Reduce to 2e-5 for long training runs\n",
    "#         logging_steps = 1,\n",
    "#         optim = \"adamw_8bit\",\n",
    "#         weight_decay = 0.01,\n",
    "#         lr_scheduler_type = \"linear\",\n",
    "#         seed = 3407,\n",
    "#         report_to = \"none\", # Use this for WandB etc\n",
    "#         dataset_num_proc=2,\n",
    "#     ),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4ea66e-49c2-4660-8c54-fa3e3a6cf5a7",
   "metadata": {},
   "source": [
    "### unsloth CLI: \n",
    "https://github.com/unslothai/unsloth/blob/main/unsloth-cli.py#L121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f831fd59-fc02-40c8-9ac9-ddf17d207264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be55b9fe-8624-434f-9f74-5eee15f1df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=5,\n",
    "    max_steps=300,\n",
    "    learning_rate=2e-3,\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    logging_steps=10,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs\",\n",
    "    report_to = \"none\", # Use this for WandB = \"wandb\", etc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "52c07117-58ae-4a45-863f-580b7f0cea61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd66f07c704418d8df73894c4c86308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False, #  Whether to pack multiple sequences into a fixed-length format. Uses max_length to define sequence length.\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4324bd98-b2e7-423a-950b-2be2601b1014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06665984-6c07-43e6-9144-c3bb8fc8d471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA RTX 6000 Ada Generation. Max memory = 47.5 GB.\n",
      "4.955 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49738d79-6ec7-4280-b118-4ab4dd931616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1c6964fe-5e5e-47db-aa96-f5ab0ae183e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 500 | Num Epochs = 2 | Total steps = 300\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 2 x 1) = 2\n",
      " \"-____-\"     Trainable parameters = 14,901,248/4,000,000,000 (0.37% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 03:37, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.132300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.328100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.555700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.408400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.422700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.536300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.449900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.355100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.466200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.368600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.432900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.344000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.320800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.315100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.287200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.322500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.414800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.232900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.256500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.272400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.276800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.261900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.333300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.862800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.901900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.881700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.896100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.889300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()\n",
    "# To resume a training run, set trainer.train(resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a8dfff-d8e2-4e05-83d1-9f06d3c2dfb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e5a2b606-638f-4f41-908a-4ee957ae2f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218.6411 seconds used for training.\n",
      "3.64 minutes used for training.\n",
      "Peak reserved memory = 4.955 GB.\n",
      "Peak reserved memory for training = 0.0 GB.\n",
      "Peak reserved memory % of max memory = 10.432 %.\n",
      "Peak reserved memory for training % of max memory = 0.0 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119aa54e-7c5f-4436-856e-0933d48410cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65230fbf-cc68-4883-9222-edc2a5e9982b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0d4d811-fdaa-4d63-a981-8d1b21330aaa",
   "metadata": {},
   "source": [
    "# Infer on FT Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "71410af9-2ad5-4a4f-a6c3-4a913ed274e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma3ForConditionalGeneration(\n",
       "      (vision_tower): SiglipVisionModel(\n",
       "        (vision_model): SiglipVisionTransformer(\n",
       "          (embeddings): SiglipVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "            (position_embedding): Embedding(4096, 1152)\n",
       "          )\n",
       "          (encoder): SiglipEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-26): 27 x SiglipEncoderLayer(\n",
       "                (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                (self_attn): SiglipAttention(\n",
       "                  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): SiglipMLP(\n",
       "                  (activation_fn): PytorchGELUTanh()\n",
       "                  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (multi_modal_projector): Gemma3MultiModalProjector(\n",
       "        (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "      )\n",
       "      (language_model): Gemma3ForCausalLM(\n",
       "        (model): Gemma3TextModel(\n",
       "          (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x Gemma3DecoderLayer(\n",
       "              (self_attn): Gemma3Attention(\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Gemma3MLP(\n",
       "                (gate_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=10240, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            )\n",
       "            (2): Gemma3DecoderLayer(\n",
       "              (self_attn): Gemma3Attention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Gemma3MLP(\n",
       "                (gate_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=10240, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            )\n",
       "            (3-6): 4 x Gemma3DecoderLayer(\n",
       "              (self_attn): Gemma3Attention(\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Gemma3MLP(\n",
       "                (gate_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=10240, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            )\n",
       "            (7-33): 27 x Gemma3DecoderLayer(\n",
       "              (self_attn): Gemma3Attention(\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Gemma3MLP(\n",
       "                (gate_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=10240, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            )\n",
       "          )\n",
       "          (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (rotary_emb): Gemma3RotaryEmbedding()\n",
       "          (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "        )\n",
       "        (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastModel.for_inference(model)  # Unsloth has 2x faster inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "16be19a4-3d06-4507-9764-1c9b762a1053",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "There is a Question provided below. It has an instruction that describes a task, paired with the input that provides further context to the task.\n",
    "Write a response that appropriately completes the request.\n",
    "\n",
    "Before providing the response, think about the question carefully and then create a step-by-step chain of thoughts to ensure that the response is logical and accurate.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "{}\n",
    "\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "36794776-6a04-4b97-b6fd-179944822b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Please answer the given financial question based on the context.\n",
    "Context: amortization expense , which is included in selling , general and administrative expenses , was $ 13.0 million , $ 13.9 million and $ 8.5 million for the years ended december 31 , 2016 , 2015 and 2014 , respectively . the following is the estimated amortization expense for the company 2019s intangible assets as of december 31 , 2016 : ( in thousands ) .\n",
    "|2017|$ 10509|\n",
    "|2018|9346|\n",
    "|2019|9240|\n",
    "|2020|7201|\n",
    "|2021|5318|\n",
    "|2022 and thereafter|16756|\n",
    "|amortization expense of intangible assets|$ 58370|\n",
    "at december 31 , 2016 , 2015 and 2014 , the company determined that its goodwill and indefinite- lived intangible assets were not impaired . 5 . credit facility and other long term debt credit facility the company is party to a credit agreement that provides revolving commitments for up to $ 1.25 billion of borrowings , as well as term loan commitments , in each case maturing in january 2021 . as of december 31 , 2016 there was no outstanding balance under the revolving credit facility and $ 186.3 million of term loan borrowings remained outstanding . at the company 2019s request and the lender 2019s consent , revolving and or term loan borrowings may be increased by up to $ 300.0 million in aggregate , subject to certain conditions as set forth in the credit agreement , as amended . incremental borrowings are uncommitted and the availability thereof , will depend on market conditions at the time the company seeks to incur such borrowings . the borrowings under the revolving credit facility have maturities of less than one year . up to $ 50.0 million of the facility may be used for the issuance of letters of credit . there were $ 2.6 million of letters of credit outstanding as of december 31 , 2016 . the credit agreement contains negative covenants that , subject to significant exceptions , limit the ability of the company and its subsidiaries to , among other things , incur additional indebtedness , make restricted payments , pledge their assets as security , make investments , loans , advances , guarantees and acquisitions , undergo fundamental changes and enter into transactions with affiliates . the company is also required to maintain a ratio of consolidated ebitda , as defined in the credit agreement , to consolidated interest expense of not less than 3.50 to 1.00 and is not permitted to allow the ratio of consolidated total indebtedness to consolidated ebitda to be greater than 3.25 to 1.00 ( 201cconsolidated leverage ratio 201d ) . as of december 31 , 2016 , the company was in compliance with these ratios . in addition , the credit agreement contains events of default that are customary for a facility of this nature , and includes a cross default provision whereby an event of default under other material indebtedness , as defined in the credit agreement , will be considered an event of default under the credit agreement . borrowings under the credit agreement bear interest at a rate per annum equal to , at the company 2019s option , either ( a ) an alternate base rate , or ( b ) a rate based on the rates applicable for deposits in the interbank market for u.s . dollars or the applicable currency in which the loans are made ( 201cadjusted libor 201d ) , plus in each case an applicable margin . the applicable margin for loans will .\n",
    "Question: what portion of the estimated amortization expense will be recognized in 2017?\n",
    "Answer:\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e7d11575-5e3f-4284-8940-ac7a3c5b27a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_format = prompt.format(question, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "26e5853a-41c9-4053-83bb-66d8e8766255",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([prompt_format], return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3825d438-23eb-4d50-aba3-710e936258e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1024,\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "83d69c57-9a8f-4799-9941-4897e11cd67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "96a99297-3016-4949-be4c-e41bce434a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<bos>There is a Question provided below. It has an instruction that describes a task, paired with the input that provides further context to the task.\\nWrite a response that appropriately completes the request.\\n\\nBefore providing the response, think about the question carefully and then create a step-by-step chain of thoughts to ensure that the response is logical and accurate.\\n\\n### Question:\\nPlease answer the given financial question based on the context.\\nContext: amortization expense , which is included in selling , general and administrative expenses , was $ 13.0 million , $ 13.9 million and $ 8.5 million for the years ended december 31 , 2016 , 2015 and 2014 , respectively . the following is the estimated amortization expense for the company 2019s intangible assets as of december 31 , 2016 : ( in thousands ) .\\n|2017|$ 10509|\\n|2018|9346|\\n|2019|9240|\\n|2020|7201|\\n|2021|5318|\\n|2022 and thereafter|16756|\\n|amortization expense of intangible assets|$ 58370|\\nat december 31 , 2016 , 2015 and 2014 , the company determined that its goodwill and indefinite- lived intangible assets were not impaired . 5 . credit facility and other long term debt credit facility the company is party to a credit agreement that provides revolving commitments for up to $ 1.25 billion of borrowings , as well as term loan commitments , in each case maturing in january 2021 . as of december 31 , 2016 there was no outstanding balance under the revolving credit facility and $ 186.3 million of term loan borrowings remained outstanding . at the company 2019s request and the lender 2019s consent , revolving and or term loan borrowings may be increased by up to $ 300.0 million in aggregate , subject to certain conditions as set forth in the credit agreement , as amended . incremental borrowings are uncommitted and the availability thereof , will depend on market conditions at the time the company seeks to incur such borrowings . the borrowings under the revolving credit facility have maturities of less than one year . up to $ 50.0 million of the facility may be used for the issuance of letters of credit . there were $ 2.6 million of letters of credit outstanding as of december 31 , 2016 . the credit agreement contains negative covenants that , subject to significant exceptions , limit the ability of the company and its subsidiaries to , among other things , incur additional indebtedness , make restricted payments , pledge their assets as security , make investments , loans , advances , guarantees and acquisitions , undergo fundamental changes and enter into transactions with affiliates . the company is also required to maintain a ratio of consolidated ebitda , as defined in the credit agreement , to consolidated interest expense of not less than 3.50 to 1.00 and is not permitted to allow the ratio of consolidated total indebtedness to consolidated ebitda to be greater than 3.25 to 1.00 ( 201cconsolidated leverage ratio 201d ) . as of december 31 , 2016 , the company was in compliance with these ratios . in addition , the credit agreement contains events of default that are customary for a facility of this nature , and includes a cross default provision whereby an event of default under other material indebtedness , as defined in the credit agreement , will be considered an event of default under the credit agreement . borrowings under the credit agreement bear interest at a rate per annum equal to , at the company 2019s option , either ( a ) an alternate base rate , or ( b ) a rate based on the rates applicable for deposits in the interbank market for u.s . dollars or the applicable currency in which the loans are made ( 201cadjusted libor 201d ) , plus in each case an applicable margin . the applicable margin for loans will .\\nQuestion: what portion of the estimated amortization expense will be recognized in 2017?\\nAnswer:\\n\\n### Response:\\n<think>\\nLet's try to figure out how much of the 2017 amortization expense we're going to see. We're told that total amortization expense is $58,370,000. This number includes various years, but we're interested in the specific year 2017. Hmm, let's see what the 2017 amortization expense is directly stated as. Oh, it says here it's $10,509,000. Now, I wonder, how does this $10,509,000 compare to the total?\\n\\nFirst, let's do a quick check of the numbers to make sure everything lines up. The total amortization expense, $58,370,000, seems to be correct from what we've seen so far. And the amount for 2017 is indeed $10,509,000. We want to find out the percentage that 2017 contributes to the entire amount.\\n\\nNow, to find the portion 2017 makes up, we can take the 2017 amortization expense and divide it by the total. So, that means $10,509,000 divided by $58,370,000. Oh, let's see what that gives us. It's about 0.18178. That’s quite close to 18%. \\n\\nTo understand this better, maybe we should think about it like this: if we take 1 and subtract the fraction, it's like saying, if you start with a whole pie, we're taking a slice that's about 1/6 of the pie. In this case, it's about 1/6 of the total pie.\\n\\nSo, that makes sense. It means in 2017, the amortization expense covers about 18% of the total $58,370,000. Just to wrap it up, we have a nice picture of how much of the total expense is accounted for in that single year. That's quite clear and direct.\\n</think>\\nIn 2017, the company will recognize an amortization expense of $10,509,000, which represents approximately 18% ( 18 % ) of the total estimated amortization expense of $58,370,000.<end_of_turn>\"]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259fd353-a2d0-4db8-9aec-4a0a28ec33c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ea3b0d-f808-4082-af94-42e8697574fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bdd8bad9-c5bd-45dd-9c4e-26638cb50953",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Given two integers `x` and `y`, can you write a python function that returns the greatest common divisor of `x` and `y`?\"\n",
    "\n",
    "prompt_format = prompt.format(question, \"\")\n",
    "\n",
    "inputs = tokenizer([prompt_format], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1500,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bc73c3db-f5d3-4fdd-be0f-57db16625a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos>There is a Question provided below. It has an instruction that describes a task, paired with the input that provides further context to the task.\\nWrite a response that appropriately completes the request.\\n\\nBefore providing the response, think about the question carefully and then create a step-by-step chain of thoughts to ensure that the response is logical and accurate.\\n\\n### Question:\\nGiven two integers `x` and `y`, can you write a python function that returns the greatest common divisor of `x` and `y`?\\n\\n### Response:\\n<think>\\nOkay, let\\'s figure out a way to find the greatest common divisor (GCD) of two numbers in Python. We\\'re looking at numbers `x` and `y` that we want to find the GCD of.\\n\\nFirst, I can think about the traditional way to find the GCD. This is the method of dividing, assuming both numbers are positive integers. We start by dividing `x` by `y` and taking the remainder, which I\\'ll store in a variable called `r`.\\n\\nNext, I\\'ll take `y` and divide it by the remainder `r`. The new remainder will be stored in a variable called `temp_r`.\\n\\nThen, I\\'ll take the previous remainder, `r`, and divide it by the new remainder, `temp_r`. This creates another remainder, `temp_r_2`, and I\\'ll store it in a variable called `temp_r_2`.\\n\\nLet\\'s keep going with this process. Each time I divide, I\\'ll update the remainder until the remainder is 0. In this case, when `temp_r_2` becomes 0, the previous remainder, `temp_r_1`, is the GCD.\\n\\nOh, so far so good! It looks like the remainder is indeed the GCD whenever we find a remainder of 0. That means the GCD is the largest number that can divide both `x` and `y` without any remainder.\\n\\nLet\\'s wrap this up. The function should take two integers as input, divide them one by one, update the remainders, and finally return the last non-zero remainder as the GCD.\\n\\nOh, wait, there\\'s a little trick to make this more efficient. Instead of updating the variables `r`, `temp_r`, and `temp_r_2` in each iteration, we can just keep track of the GCD we found so far.\\n\\nWhen we divide `x` by `y` and get a remainder, we\\'ll compare the current GCD with the remainder. If the remainder is larger than the current GCD, I\\'ll update the GCD. This way, the GCD will always be the largest number that\\'s a divisor of both numbers.\\n\\nOh, that\\'s a nice trick to streamline the process. Once the remainder is 0, I just return the GCD I\\'ve been tracking. Seems like the function is ready to go now!\\n\\nAlright, so to recap:\\n- Take two inputs, `x` and `y`.\\n- Start by dividing `x` by `y` and storing the remainder.\\n- Assume the current remainder is the GCD.\\n- Divide `y` by the remainder and store the new remainder.\\n- If the new remainder is 0, return the current GCD.\\n- Otherwise, compare the current GCD with the new remainder and update the GCD accordingly.\\n- Continue this until the remainder becomes 0, then return the GCD.\\n</think>\\n```python\\ndef gcd(x, y):\\n    \"\"\"\\n    Finds the greatest common divisor of two integers.\\n    \"\"\"\\n    while(y):\\n        x, y = y, x % y\\n    return x\\n```\\nThis function `gcd(x, y)` takes two integers as input, `x` and `y`, and returns their greatest common divisor.\\n\\nIt works by repeatedly dividing the larger number by the smaller number and updating the smaller number with the remainder until the remainder is zero. At that point, the smaller number is the greatest common divisor.\\n\\nWhen the smaller number becomes zero, it means the division is complete, and the GCD is the larger number, `x`, which is now assigned to `y`.\\n\\nSo, if `x = 10` and `y = 6`, the function proceeds as follows:\\n1. Divide 10 by 6: $10 = 1 \\\\cdot 6 + 4$\\n2. Update y to 4.\\n3. Divide 6 by 4: $6 = 1 \\\\cdot 4 + 2$\\n4. Update y to 2.\\n5. Divide 4 by 2: $4 = 2 \\\\cdot 2 + 0$\\n6. Since the remainder is 0, the GCD is 2.\\n\\nThis function correctly returns the GCD of 10 and 6, which is 2. It demonstrates a straightforward and efficient method for finding the GCD of two integers using the Euclidean algorithm.\\n<end_of_turn>']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "00b7cfdc-c66e-4adc-a745-a5aa58437aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Okay, let's figure out a way to find the greatest common divisor (GCD) of two numbers in Python. We're looking at numbers `x` and `y` that we want to find the GCD of.\n",
      "\n",
      "First, I can think about the traditional way to find the GCD. This is the method of dividing, assuming both numbers are positive integers. We start by dividing `x` by `y` and taking the remainder, which I'll store in a variable called `r`.\n",
      "\n",
      "Next, I'll take `y` and divide it by the remainder `r`. The new remainder will be stored in a variable called `temp_r`.\n",
      "\n",
      "Then, I'll take the previous remainder, `r`, and divide it by the new remainder, `temp_r`. This creates another remainder, `temp_r_2`, and I'll store it in a variable called `temp_r_2`.\n",
      "\n",
      "Let's keep going with this process. Each time I divide, I'll update the remainder until the remainder is 0. In this case, when `temp_r_2` becomes 0, the previous remainder, `temp_r_1`, is the GCD.\n",
      "\n",
      "Oh, so far so good! It looks like the remainder is indeed the GCD whenever we find a remainder of 0. That means the GCD is the largest number that can divide both `x` and `y` without any remainder.\n",
      "\n",
      "Let's wrap this up. The function should take two integers as input, divide them one by one, update the remainders, and finally return the last non-zero remainder as the GCD.\n",
      "\n",
      "Oh, wait, there's a little trick to make this more efficient. Instead of updating the variables `r`, `temp_r`, and `temp_r_2` in each iteration, we can just keep track of the GCD we found so far.\n",
      "\n",
      "When we divide `x` by `y` and get a remainder, we'll compare the current GCD with the remainder. If the remainder is larger than the current GCD, I'll update the GCD. This way, the GCD will always be the largest number that's a divisor of both numbers.\n",
      "\n",
      "Oh, that's a nice trick to streamline the process. Once the remainder is 0, I just return the GCD I've been tracking. Seems like the function is ready to go now!\n",
      "\n",
      "Alright, so to recap:\n",
      "- Take two inputs, `x` and `y`.\n",
      "- Start by dividing `x` by `y` and storing the remainder.\n",
      "- Assume the current remainder is the GCD.\n",
      "- Divide `y` by the remainder and store the new remainder.\n",
      "- If the new remainder is 0, return the current GCD.\n",
      "- Otherwise, compare the current GCD with the new remainder and update the GCD accordingly.\n",
      "- Continue this until the remainder becomes 0, then return the GCD.\n",
      "</think>\n",
      "```python\n",
      "def gcd(x, y):\n",
      "    \"\"\"\n",
      "    Finds the greatest common divisor of two integers.\n",
      "    \"\"\"\n",
      "    while(y):\n",
      "        x, y = y, x % y\n",
      "    return x\n",
      "```\n",
      "This function `gcd(x, y)` takes two integers as input, `x` and `y`, and returns their greatest common divisor.\n",
      "\n",
      "It works by repeatedly dividing the larger number by the smaller number and updating the smaller number with the remainder until the remainder is zero. At that point, the smaller number is the greatest common divisor.\n",
      "\n",
      "When the smaller number becomes zero, it means the division is complete, and the GCD is the larger number, `x`, which is now assigned to `y`.\n",
      "\n",
      "So, if `x = 10` and `y = 6`, the function proceeds as follows:\n",
      "1. Divide 10 by 6: $10 = 1 \\cdot 6 + 4$\n",
      "2. Update y to 4.\n",
      "3. Divide 6 by 4: $6 = 1 \\cdot 4 + 2$\n",
      "4. Update y to 2.\n",
      "5. Divide 4 by 2: $4 = 2 \\cdot 2 + 0$\n",
      "6. Since the remainder is 0, the GCD is 2.\n",
      "\n",
      "This function correctly returns the GCD of 10 and 6, which is 2. It demonstrates a straightforward and efficient method for finding the GCD of two integers using the Euclidean algorithm.\n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0764f72e-747a-4a7e-87f7-29366495d7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1fa8ec26-76dd-460f-9c9f-0825cbbe903a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Alright, so let's try to figure this out. I need to take an integer, which I'll call 'n', and then find the corresponding Fibonacci number.\n",
      "\n",
      "First, I need to understand what the Fibonacci numbers are. These numbers are part of a sequence where each number in the sequence is the sum of the previous two numbers. So, if I take a starting number, let's say 0, and then add the previous number, which is also 0, to get the first number in the sequence, then 0 plus 0 gives me 0. If I start with 1 and add the previous number, 1, I get 1. That's the sequence: 0, 1, 1, 2, 3, 5, and so on.\n",
      "\n",
      "Now, how do I find the n-th Fibonacci number? To do that, I can use a simple recursive approach. So, let's set up a function that takes 'n' as an argument. The function checks if 'n' is 0. If it is, we return 0, otherwise if 'n' is 1, we return 1. If 'n' is greater than 1, we call the function again to find the (n-1)-th Fibonacci number. Then, we add the (n-1)-th Fibonacci number to the (n-2)-th Fibonacci number. This keeps going until we get to the first two numbers, 0 and 1.\n",
      "\n",
      "Let's see how it works with some examples. If n = 0, the function returns 0, which makes sense since 0 is the 0th number in the Fibonacci sequence. If n = 1, we return 1, as the sequence starts with 0 and 1. If n = 2, we call the function again, get 1 for n-1 and 0 for n-2, so we sum 1 and 0, which gives 1. Then we get 1 for n-2 and 1 for n-3, summing them gives 2. And so on.\n",
      "\n",
      "But this approach might be a bit slow for larger values of 'n' because of the repeated calls. To improve the performance, I could consider using dynamic programming. With dynamic programming, we'd store the Fibonacci numbers we've already computed in a table. So, when I need to find the n-th Fibonacci number, I first look in the table. If it's already there, I just return the stored value. If not, I calculate it using the recursive formula and add it to the table before returning it. This avoids recomputing the same numbers over and over again.\n",
      "\n",
      "Okay, let's wrap it up. With the recursive approach, I take the input 'n' and check for base cases of 0 and 1. For 'n' greater than 1, I recursively call the function with n-1, and then add the result to the value for n-2. Finally, I return the calculated Fibonacci number.\n",
      "\n",
      "Just to make sure everything's clear, the given task is to find the n-th Fibonacci number using a recursive approach.\n",
      "</think>\n",
      "The Fibonacci numbers can be defined as follows: F(0) = 0, F(1) = 1, and F(n) = F(n-1) + F(n-2) for n > 1.\n",
      "\n",
      "To find the n-th Fibonacci number, I would typically use a recursive approach, similar to the one you described. This involves creating a function that takes an integer 'n' as input and then defines a base case for n = 0 and n = 1. For larger 'n' values, the function then recursively calls itself to compute F(n) by summing the Fibonacci numbers F(n-1) and F(n-2).\n",
      "\n",
      "Now, let's consider how this recursion works step by step.\n",
      "\n",
      "First, if n is 0, the function returns 0, which is the first number in the Fibonacci sequence. For n = 1, the function returns 1, because it's the second number.\n",
      "\n",
      "Next, when n is greater than 1, the function checks if F(n-1) is already computed and stored in the table (or memoized). If it is, the function simply returns F(n-1). If F(n-1) is not computed, the function calls itself with n-1 as the input, expecting to get F(n-1) back from this new call.\n",
      "\n",
      "After getting F(n-1) from this call, it then calls itself again with n-2 as the argument, hoping to retrieve F(n-2). Finally, if both F(n-1) and F(n-2) have been found, the function returns F(n) = F(n-1) + F(n-2).\n",
      "\n",
      "So, in essence, this recursive method efficiently traverses the Fibonacci sequence by repeatedly applying the definition F(n) = F(n-1) + F(n-2) until it reaches the base cases.\n",
      "The code for this approach would look like:\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "# Example\n",
      "fibonacci(5)\n",
      "```\n",
      "\n",
      "This code provides a clear step-by-step illustration of how the recursive Fibonacci function works.\n",
      "\n",
      "Now, let me think about this for a while. Perhaps using memoization to store already calculated values could make this recursive solution more efficient, especially for larger numbers. But for now, I'm happy with this straightforward approach to find the n-th Fibonacci number.<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "question = \"Given an integer number 'n', write python code to give me fibonacii number?\"\n",
    "\n",
    "prompt_format = prompt.format(question, \"\")\n",
    "\n",
    "inputs = tokenizer([prompt_format], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=2000,\n",
    "    use_cache=True,\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "\n",
    "\n",
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e81a910-37a6-4d25-9be1-c141c78eb377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df34c5b0-289e-48c5-a335-d45581b67fa8",
   "metadata": {},
   "source": [
    "# Save and Push to HUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "56770175-a073-4e74-9c23-312ad242e3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gemma-3-4B-Fin-QA-Reasoning/processor_config.json']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Locally - Adapters Only\n",
    "new_model_local = \"Gemma-3-4B-Fin-QA-Reasoning\"\n",
    "model.save_pretrained(new_model_local) \n",
    "tokenizer.save_pretrained(new_model_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0568b578-8a51-42f4-a642-e40cc6fd7194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading safetensors index for unsloth/gemma-3-4b-it...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b407919247ed4790b89a4b301051308b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d285abf216904b449569c862da4da2d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14dbd3aed7484ee79ee578ebca0e47a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  50%|█████     | 1/2 [01:27<01:27, 87.31s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a531ae81df914ff0838f302dd0e96b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|██████████| 2/2 [02:27<00:00, 73.55s/it]\n"
     ]
    }
   ],
   "source": [
    "# Save Locally - Merged Model\n",
    "new_ft_model = \"Rathsam/Gemma-3-4B-Financial-QA-Reasoning\"\n",
    "\n",
    "if True:\n",
    "    model.save_pretrained_merged(new_ft_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b76ba613-aed9-46a1-b523-452a043eb719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_ft_model = \"Rathsam/Gemma-3-4B-Fin-QA-Reasoning\"\n",
    "\n",
    "# if True: # Change to True to upload finetune\n",
    "#     model.push_to_hub_merged(\n",
    "#         new_ft_model, tokenizer,\n",
    "#         token = \"\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5194a3da-e429-4753-aa04-6e04e4fbd5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54375bc1-d75e-4b06-9821-2316c9b23796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d14841a-7f3b-4d1d-8202-19b87b16b447",
   "metadata": {},
   "source": [
    "# Inference on merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "142eda51-4cd9-4cbc-8ece-534084e05dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.7: Fast Gemma3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA RTX 6000 Ada Generation. Num GPUs = 1. Max memory: 47.5 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8b97b240264a8ebbf99b3f93344c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rathsam/Gemma-3-4B-Fin-QA-Reasoning were not used when initializing Gemma3ForCausalLM: ['multi_modal_projector.mm_input_projection_weight', 'multi_modal_projector.mm_soft_emb_norm.weight', 'vision_tower.vision_model.embeddings.patch_embedding.bias', 'vision_tower.vision_model.embeddings.patch_embedding.weight', 'vision_tower.vision_model.embeddings.position_embedding.weight', 'vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.24.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.24.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.24.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.24.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.24.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.24.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.24.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.24.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.25.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.25.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.25.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.25.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.25.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.25.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.25.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.25.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.26.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.26.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.26.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.26.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.26.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.26.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.26.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.26.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_tower.vision_model.post_layernorm.bias', 'vision_tower.vision_model.post_layernorm.weight']\n",
      "- This IS expected if you are initializing Gemma3ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Gemma3ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"Rathsam/Gemma-3-4B-Fin-QA-Reasoning\",\n",
    "    max_seq_length = 2048, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fef30858-efa8-43a5-9f49-58b6f27bc714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Alright, let's figure out how to find the Fibonacci number for a given number 'n'.\n",
      "\n",
      "First, I need to understand what the Fibonacci sequence is all about. It starts with 0 and 1, and each subsequent number is the sum of the two preceding ones. So, if I have 0 and 1, the next number is 1, then 1 plus 1 is 2, and so on.\n",
      "\n",
      "Now, let's think about how to find the Fibonacci number for a specific 'n'. I can do this by creating a function that takes 'n' as input and returns the Fibonacci number for that 'n'.\n",
      "\n",
      "Let's start by defining the base cases. If n is 0, the Fibonacci number is 0. If n is 1, the Fibonacci number is 1. Got it.\n",
      "\n",
      "Next, I need to define a recursive function to calculate the Fibonacci number. This function should take 'n' as input and return the Fibonacci number for 'n'.\n",
      "\n",
      "Let's write this down:\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "```\n",
      "\n",
      "This function works by calling itself with smaller values of 'n' until it reaches the base cases. So, if I call `fibonacci(5)`, it will call `fibonacci(4)` and `fibonacci(3)`, and so on, until it gets to 0 and 1.\n",
      "\n",
      "Now, let's see how this works for a given 'n'. If I call `fibonacci(5)`, it will do the following:\n",
      "\n",
      "1.  It checks if n is 0 or 1. Since 5 is neither 0 nor 1, it goes to the `else` part.\n",
      "2.  It then calls `fibonacci(4)` and `fibonacci(3)`.\n",
      "3.  Let's see what happens when it calls `fibonacci(4)`. It calls `fibonacci(3)` and `fibonacci(2)`.\n",
      "4.  When it calls `fibonacci(3)`, it calls `fibonacci(2)` and `fibonacci(1)`.\n",
      "5.  When it calls `fibonacci(2)`, it calls `fibonacci(1)` and `fibonacci(0)`.\n",
      "6.  When it calls `fibonacci(1)`, it returns 1.\n",
      "7.  When it calls `fibonacci(0)`, it returns 0.\n",
      "8.  So, `fibonacci(4)` returns 1 + 0 = 1.\n",
      "9.  Then, `fibonacci(3)` returns 1 + 1 = 2.\n",
      "10. Finally, `fibonacci(5)` returns 2 + 1 = 3.\n",
      "\n",
      "That's how it works! The function calculates the Fibonacci number by summing the two preceding numbers in the sequence.\n",
      "\n",
      "Oh, wait, there's a more efficient way to do this. Instead of using recursion, I can use an iterative approach. This will be faster because it avoids the overhead of function calls.\n",
      "\n",
      "Let's try an iterative approach:\n",
      "```python\n",
      "def fibonacci_iterative(n):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        a, b = 0, 1\n",
      "        for _ in range(2, n + 1):\n",
      "            a, b = b, a + b\n",
      "        return b\n",
      "```\n",
      "\n",
      "This iterative approach is simpler and more efficient than the recursive one. It initializes two variables, 'a' and 'b', with the first two Fibonacci numbers, 0 and 1. Then, it iterates from 2 up to 'n', updating 'a' and 'b' in each iteration to calculate the next Fibonacci number.\n",
      "\n",
      "Let's see how this works for `n = 5`:\n",
      "\n",
      "1.  It starts with a = 0 and b = 1.\n",
      "2.  In the first iteration, it updates a to 1 and b to 1.\n",
      "3.  In the second iteration, it updates a to 1 and b to 2.\n",
      "4.  In the third iteration, it updates a to 2 and b to 3.\n",
      "5.  In the fourth iteration, it updates a to 3 and b to 5.\n",
      "6.  In the fifth iteration, it updates a to 5 and b to 8.\n",
      "7.  Finally, it returns the value of b, which is 8.\n",
      "\n",
      "So, `fibonacci_iterative(5)` returns 8, which is the 5th Fibonacci number.\n",
      "\n",
      "Now, let's compare the recursive and iterative approaches. The recursive approach is straightforward but can be inefficient for large values of 'n' because it performs many redundant calculations. The iterative approach is more efficient because it calculates each Fibonacci number only once.\n",
      "\n",
      "In conclusion, the iterative approach is generally preferred for calculating Fibonacci numbers, especially for larger values of 'n', due to its efficiency.\n",
      "</think>\n",
      "The Fibonacci number for n=5 is 8. This was calculated using the iterative approach, which is more efficient than the recursive approach for larger values of n.\n",
      "<end_of_turn>\n",
      "<end_of_turn><end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Given an integer number 'n', write python code to give me fibonacii number?\"\n",
    "\n",
    "prompt_format = prompt.format(question, \"\")\n",
    "\n",
    "inputs = tokenizer([prompt_format], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "\n",
    "\n",
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be58ce6-b47d-4bca-ac55-a2d9cadf2141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7752ce-e580-4014-84c8-4539b2b02832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "93f3dda7-2a57-48f7-8f10-05313a170dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rebel-admiral-1</strong> at: <a href='https://wandb.ai/laravelshubham-student/Fine-tuning%20Gemma-3-4B%20on%20FinQA%20Reasoning%20Dataset/runs/m2grhgzs?apiKey=9cec55360d515cebf3a98e5f8de569b5c44f6265' target=\"_blank\">https://wandb.ai/laravelshubham-student/Fine-tuning%20Gemma-3-4B%20on%20FinQA%20Reasoning%20Dataset/runs/m2grhgzs?apiKey=9cec55360d515cebf3a98e5f8de569b5c44f6265</a><br> View project at: <a href='https://wandb.ai/laravelshubham-student/Fine-tuning%20Gemma-3-4B%20on%20FinQA%20Reasoning%20Dataset?apiKey=9cec55360d515cebf3a98e5f8de569b5c44f6265' target=\"_blank\">https://wandb.ai/laravelshubham-student/Fine-tuning%20Gemma-3-4B%20on%20FinQA%20Reasoning%20Dataset?apiKey=9cec55360d515cebf3a98e5f8de569b5c44f6265</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250504_190702-m2grhgzs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738fa2db-8873-46f9-a6a1-165b1b086979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
